[
  {
    "query": "Maximum average subarray",
    "chunk_scores": [
      {
        "text": "tags: - Translated e_maxx_link: maximum_average_segment\n\nSearch the subarray with the maximum/minimum sum\n\nHere, we consider the problem of finding a subarray with maximum sum, as well as some of its variations (including the algorithm for solving this problem online).\n\nProblem statement\n\nGiven an array of numbers $a[1 \\ldots n]$. It is required to find a subarray $a[l \\ldots r]$ with the maximal sum:\n\n$$ \\max_{ 1 \\le l \\le r \\le n } \\sum_{i=l}^{r} a[i].$$",
        "score": 0.6816357374191284,
        "is_relevant": true
      },
      {
        "text": "Search for a subarray with a maximum/minimum average\n\nThis problem lies in finding such a segment $a[l, r]$, such that the average value is maximal:\n\n$$ \\max_{l \\le r} \\frac{ 1 }{ r-l+1 } \\sum_{i=l}^{r} a[i].$$",
        "score": 0.6861545443534851,
        "is_relevant": true
      },
      {
        "text": "In this case, we apply the standard technique when working with the problems of the average value: we will select the desired maximum average value by binary search.\n\nTo do this, we need to learn how to solve the following subproblem: given the number $x$, and we need to check whether there is a subarray of array $a[]$ (of course, satisfying all additional constraints of the problem), where the average value is greater than $x$.",
        "score": 0.6892515420913696,
        "is_relevant": true
      },
      {
        "text": "Problem statement\n\nGiven an array of numbers $a[1 \\ldots n]$. It is required to find a subarray $a[l \\ldots r]$ with the maximal sum:\n\n$$ \\max_{ 1 \\le l \\le r \\le n } \\sum_{i=l}^{r} a[i].$$\n\nFor example, if all integers in array $a[]$ were non-negative, then the answer would be the array itself. However, the solution is non-trivial when the array can contain both positive and negative numbers.",
        "score": 0.7365475296974182,
        "is_relevant": true
      },
      {
        "text": "This problem lies in finding such a segment $a[l, r]$, such that the average value is maximal:\n\n$$ \\max_{l \\le r} \\frac{ 1 }{ r-l+1 } \\sum_{i=l}^{r} a[i].$$\n\nOf course, if no other conditions are imposed on the required segment $[l, r]$, then the solution will always be a segment of length $1$ at the maximum element of the array. The problem only makes sense, if there are additional restrictions (for example, the length of the desired segment is bounded below).",
        "score": 0.7728617787361145,
        "is_relevant": true
      },
      {
        "text": "Solving the online problem\n\nThe condition of the problem is as follows: given an array of $n$ numbers, and a number $L$. There are queries of the form $(l,r)$, and in response to each query, it is required to find a subarray of the segment $[l, r]$ of length not less than $L$ with the maximum possible arithmetic mean.\n\nThe algorithm for solving this problem is quite complex. KADR (Yaroslav Tverdokhleb) described his algorithm on the Russian forum.",
        "score": 0.7920325994491577,
        "is_relevant": true
      },
      {
        "text": "Algorithm description\n\nThe algorithm is very simple.\n\nWe introduce for convenience the notation: $s[i] = \\sum_{j=1}^{i} a[j]$. That is, the array $s[i]$ is an array of partial sums of array $a[]$. Also, set $s[0] = 0$.\n\nLet us now iterate over the index $r = 1 \\ldots n$, and learn how to quickly find the optimal $l$ for each current value $r$, at which the maximum sum is reached on the subarray $[l, r]$.",
        "score": 0.9032163023948669,
        "is_relevant": true
      },
      {
        "text": "Let us now iterate over the index $r = 1 \\ldots n$, and learn how to quickly find the optimal $l$ for each current value $r$, at which the maximum sum is reached on the subarray $[l, r]$.\n\nFormally, this means that for the current $r$ we need to find an $l$ (not exceeding $r$), so that the value of $s[r] - s[l-1]$ is maximal. After a trivial transformation, we can see that we need to find in the array $s[]$ a minimum on the segment $[0, r-1]$.",
        "score": 0.9117918014526367,
        "is_relevant": true
      },
      {
        "text": "Related tasks\n\nFinding the maximum/minimum subarray with constraints\n\nIf the problem condition imposes additional restrictions on the required segment $[l, r]$ (for example, that the length $r-l+1$ of the segment must be within the specified limits), then the described algorithm is likely to be easily generalized to these cases — anyway, the problem will still be to find the minimum in the array $s[]$ with the specified additional restrictions.",
        "score": 0.9264744520187378,
        "is_relevant": true
      },
      {
        "text": "Two-dimensional case of the problem: search for maximum/minimum submatrix\n\nThe problem described in this article is naturally generalized to large dimensions. For example, in a two-dimensional case, it turns into a search for such a submatrix $[l_1 \\ldots r_1, l_2 \\ldots r_2]$ of a given matrix, which has the maximum sum of numbers in it.",
        "score": 0.9911271333694458,
        "is_relevant": true
      }
    ]
  },
  {
    "query": "Finding polynomial roots modulo a prime",
    "chunk_scores": [
      {
        "text": "$x^k \\equiv a \\pmod n$\n\nThe algorithm\n\nWe will solve this problem by reducing it to the discrete logarithm problem.\n\nLet's apply the concept of a primitive root modulo $n$. Let $g$ be a primitive root modulo $n$. Note that since $n$ is prime, it must exist, and it can be found in $O(Ans \\cdot \\log \\phi (n) \\cdot \\log n) = O(Ans \\cdot \\log^2 n)$ plus time of factoring $\\phi (n)$.\n\nWe can easily discard the case where $a = 0$. In this case, obviously there is only one answer: $x = 0$.",
        "score": 0.9456417560577393,
        "is_relevant": true
      },
      {
        "text": "We can easily discard the case where $a = 0$. In this case, obviously there is only one answer: $x = 0$.\n\nSince we know that $n$ is a prime and any number between 1 and $n-1$ can be represented as a power of the primitive root, we can represent the discrete root problem as follows:\n\n$(g^y)^k \\equiv a \\pmod n$\n\nwhere\n\n$x \\equiv g^y \\pmod n$\n\nThis, in turn, can be rewritten as\n\n$(g^k)^y \\equiv a \\pmod n$",
        "score": 1.092057228088379,
        "is_relevant": true
      },
      {
        "text": "Multiplication with arbitrary modulus\n\nHere we want to achieve the same goal as in previous section. Multiplying two polynomial $A(x)$ and $B(x)$, and computing the coefficients modulo some number $M$. The number theoretic transform only works for certain prime numbers. What about the case when the modulus is not of the desired form?",
        "score": 1.1164805889129639,
        "is_relevant": true
      },
      {
        "text": "// compute a^M\n    for (int p : primes) {\n        if (p >= B)\n            continue;\n        long long p_power = 1;\n        while (p_power * p <= B)\n            p_power *= p;\n        a = power(a, p_power, n);\n\n        g = gcd(a - 1, n);\n        if (g > 1 && g < n)\n            return g;\n    }\n    B *= 2;\n}\nreturn 1;\n\n```\n\nObserve that this is a probabilistic algorithm. A consequence of this is that there is a possibility of the algorithm being unable to find a factor at all.",
        "score": 1.143733263015747,
        "is_relevant": true
      },
      {
        "text": "tags: - Translated e_maxx_link: discrete_root\n\nDiscrete Root\n\nThe problem of finding a discrete root is defined as follows. Given a prime $n$ and two integers $a$ and $k$, find all $x$ for which:\n\n$x^k \\equiv a \\pmod n$\n\nThe algorithm\n\nWe will solve this problem by reducing it to the discrete logarithm problem.",
        "score": 1.1502934694290161,
        "is_relevant": true
      },
      {
        "text": "```cpp int gcd(int a, int b) { return a ? gcd(b % a, a) : b; }\n\nint powmod(int a, int b, int p) { int res = 1; while (b > 0) { if (b & 1) { res = res * a % p; } a = a * a % p; b >>= 1; } return res; }\n\n// Finds the primitive root modulo p int generator(int p) { vector\n\nfor (int res = 2; res <= p; ++res) {\n    bool ok = true;\n    for (int factor : fact) {\n        if (powmod(res, phi / factor, p) == 1) {\n            ok = false;\n            break;\n        }\n    }\n    if (ok) return res;\n}\nreturn -1;",
        "score": 1.1538939476013184,
        "is_relevant": false
      },
      {
        "text": "the values, $1303 - 1$ and $3697 - 1$, are $31$-powersmooth and $16$-powersmooth respectively, because $1303 - 1 = 2 \\cdot 3 \\cdot 7 \\cdot 31$ and $3697 - 1 = 2^4 \\cdot 3 \\cdot 7 \\cdot 11$. In 1974 John Pollard invented a method to extract factors $p$, s.t. $p-1$ is $\\mathrm{B}$-powersmooth, from a composite number.",
        "score": 1.1540110111236572,
        "is_relevant": false
      },
      {
        "text": "So for any $M$ with $p - 1 ~|~ M$ we know that $a^M \\equiv 1$. This means that $a^M - 1 = p \\cdot r$, and because of that also $p ~|~ \\gcd(a^M - 1, n)$.\n\nTherefore, if $p - 1$ for a factor $p$ of $n$ divides $M$, we can extract a factor using Euclid's algorithm.\n\nIt is clear, that the smallest $M$ that is a multiple of every $\\mathrm{B}$-powersmooth number is $\\text{lcm}(1,~2~,3~,4~,~\\dots,~B)$. Or alternatively:\n\n$$M = \\prod_{\\text{prime } q \\le B} q^{\\lfloor \\log_q B \\rfloor}$$",
        "score": 1.1709811687469482,
        "is_relevant": true
      },
      {
        "text": "Notice, if the number that you want to factorize is actually a prime number, most of the algorithms will run very slowly. This is especially true for Fermat's, Pollard's p-1 and Pollard's rho factorization algorithms. Therefore, it makes the most sense to perform a probabilistic (or a fast deterministic) primality test before trying to factorize the number.\n\nTrial division\n\nThis is the most basic algorithm to find a prime factorization.",
        "score": 1.1721608638763428,
        "is_relevant": false
      },
      {
        "text": "$x = g^{y_0 + i \\frac {\\phi (n)}{gcd(k, \\phi (n))}} \\pmod n \\forall i \\in Z$.\n\nThis is the final formula for all solutions of the discrete root problem.\n\nImplementation\n\nHere is a full implementation, including procedures for finding the primitive root, discrete log and finding and printing all solutions.\n\n```cpp int gcd(int a, int b) { return a ? gcd(b % a, a) : b; }\n\nint powmod(int a, int b, int p) { int res = 1; while (b > 0) { if (b & 1) { res = res * a % p; } a = a * a % p; b >>= 1; } return res; }",
        "score": 1.1812392473220825,
        "is_relevant": false
      }
    ]
  },
  {
    "query": "Detecting negative cycles in graphs",
    "chunk_scores": [
      {
        "text": "It is convenient to use different algorithms to solve these two variations of the problem, so we'll discuss both of them here.\n\nUsing Bellman-Ford algorithm\n\nBellman-Ford algorithm allows you to check whether there exists a cycle of negative weight in the graph, and if it does, find one of these cycles.\n\nThe details of the algorithm are described in the article on the Bellman-Ford algorithm. Here we'll describe only its application to this problem.",
        "score": 0.5225944519042969,
        "is_relevant": true
      },
      {
        "text": "We will look for the Euler cycle exactly as described above (non-recursive version), and at the same time at the end of this algorithm we will check whether the graph was connected or not (if the graph was not connected, then at the end of the algorithm some edges will remain in the graph, and in this case we need to print $-1$). Finally, the program takes into account that there can be isolated vertices in the graph.",
        "score": 0.7468760013580322,
        "is_relevant": false
      },
      {
        "text": "The standard implementation of Bellman-Ford looks for a negative cycle reachable from some starting vertex $v$ ; however, the algorithm can be modified to just looking for any negative cycle in the graph. For this we need to put all the distance $d[i]$ to zero and not infinity — as if we are looking for the shortest path from all vertices simultaneously; the validity of the detection of a negative cycle is not affected.",
        "score": 0.7574804425239563,
        "is_relevant": true
      },
      {
        "text": "Do $N$ iterations of Bellman-Ford algorithm. If there were no changes on the last iteration, there is no cycle of negative weight in the graph. Otherwise take a vertex the distance to which has changed, and go from it via its ancestors until a cycle is found. This cycle will be the desired cycle of negative weight.\n\nImplementation\n\n```cpp struct Edge { int a, b, cost; };\n\nint n; vector\n\nvoid solve() { vector",
        "score": 0.8011718988418579,
        "is_relevant": true
      },
      {
        "text": "tags: - Translated e_maxx_link: negative_cycle\n\nFinding a negative cycle in the graph\n\nYou are given a directed weighted graph $G$ with $N$ vertices and $M$ edges. Find any cycle of negative weight in it, if such a cycle exists.\n\nIn another formulation of the problem you have to find all pairs of vertices which have a path of arbitrarily small weight between them.\n\nIt is convenient to use different algorithms to solve these two variations of the problem, so we'll discuss both of them here.",
        "score": 0.8018989562988281,
        "is_relevant": true
      },
      {
        "text": "if (x == -1) {\n    cout << \"No negative cycle found.\";\n} else {\n    for (int i = 0; i < n; ++i)\n        x = p[x];\n\n    vector<int> cycle;\n    for (int v = x;; v = p[v]) {\n        cycle.push_back(v);\n        if (v == x && cycle.size() > 1)\n            break;\n    }\n    reverse(cycle.begin(), cycle.end());\n\n    cout << \"Negative cycle: \";\n    for (int v : cycle)\n        cout << v << ' ';\n    cout << endl;\n}\n\n} ```\n\nUsing Floyd-Warshall algorithm",
        "score": 0.8280225992202759,
        "is_relevant": true
      },
      {
        "text": "Implementation\n\nHere is an implementation for directed graph.\n\n```cpp int n; vector\n\nbool dfs(int v) { color[v] = 1; for (int u : adj[v]) { if (color[u] == 0) { parent[u] = v; if (dfs(u)) return true; } else if (color[u] == 1) { cycle_end = v; cycle_start = u; return true; } } color[v] = 2; return false; }\n\nvoid find_cycle() { color.assign(n, 0); parent.assign(n, -1); cycle_start = -1;\n\nfor (int v = 0; v < n; v++) {\n    if (color[v] == 0 && dfs(v))\n        break;\n}",
        "score": 0.9228042364120483,
        "is_relevant": true
      },
      {
        "text": "Looking for all cycles and combining them can be done with a simple recursive procedure:\n\nnohighlight procedure FindEulerPath(V) 1. iterate through all the edges outgoing from vertex V; remove this edge from the graph, and call FindEulerPath from the second end of this edge; 2. add vertex V to the answer.\n\nThe complexity of this algorithm is obviously linear with respect to the number of edges.\n\nBut we can write the same algorithm in the non-recursive version:",
        "score": 0.9473800659179688,
        "is_relevant": false
      },
      {
        "text": "cout << \"Negative cycle: \";\n    for (int v : cycle)\n        cout << v << ' ';\n    cout << endl;\n}\n\n} ```\n\nUsing Floyd-Warshall algorithm\n\nThe Floyd-Warshall algorithm allows to solve the second variation of the problem - finding all pairs of vertices $(i, j)$ which don't have a shortest path between them (i.e. a path of arbitrarily small weight exists).\n\nAgain, the details can be found in the Floyd-Warshall article, and here we describe only its application.",
        "score": 0.9731334447860718,
        "is_relevant": true
      },
      {
        "text": "Here is an implementation for undirected graph. Note that in the undirected version, if a vertex v gets colored black, it will never be visited again by the DFS. This is because we already explored all connected edges of v when we first visited it. The connected component containing v (after removing the edge between v and its parent) must be a tree, if the DFS has completed processing v without finding a cycle. So we don't even need to distinguish between gray and black states. Thus we can turn the char",
        "score": 1.0072394609451294,
        "is_relevant": false
      }
    ]
  },
  {
    "query": "Solving the 15-puzzle with minimal moves",
    "chunk_scores": [
      {
        "text": "Practice Problems\n\nAtCoder - Yakiniku Restaurants\n\nCodeForces - Ciel and Gondolas (Be careful with I/O!)\n\nCodeForces - Levels And Regions\n\nCodeForces - Partition Game\n\nCodeForces - The Bakery\n\nCodeForces - Yet Another Minimization Problem\n\nCodechef - CHEFAOR\n\nCodeForces - GUARDS (This is the exact problem in this article.)\n\nHackerrank - Guardians of the Lunatics\n\nHackerrank - Mining\n\nKattis - Money (ACM ICPC World Finals 2017)\n\nSPOJ - ADAMOLD\n\nSPOJ - LARMY\n\nSPOJ - NKLEAVES\n\nTimus - Bicolored Horses",
        "score": 1.2256748676300049,
        "is_relevant": false
      },
      {
        "text": "Let $d[v]$ be the answer for the node $v$, i.e. we already processed part of the substring, are currently in the state $v$, and want to find the smallest number of characters that have to be added to find a non-existent transition. Computing $d[v]$ is very simple. If there is not transition using at least one character of the alphabet, then $d[v] = 1$. Otherwise one character is not enough, and so we need to take the minimum of all answers of all transitions:\n\n$$d[v] = 1 + \\min_{w:(v,w,c) \\in SA} d[w].$$",
        "score": 1.2539641857147217,
        "is_relevant": false
      },
      {
        "text": "Now we only have to learn how to quickly answer range minimum queries within each block. In fact if the received range minimum query is $[l, r]$ and $l$ and $r$ are in different blocks then the answer is the minimum of the following three values: the minimum of the suffix of block of $l$ starting at $l$, the minimum of the prefix of block of $r$ ending at $r$, and the minimum of the blocks between those. The minimum of the blocks in between can be answered in $O(1)$ using the Sparse Table. So this leaves",
        "score": 1.2752277851104736,
        "is_relevant": false
      },
      {
        "text": "Shortest non-appearing string\n\nGiven a string $S$ and a certain alphabet. We have to find a string of the smallest length, that doesn't appear in $S$.\n\nWe will apply dynamic programming on the suffix automaton built for the string $S$.",
        "score": 1.2864036560058594,
        "is_relevant": false
      },
      {
        "text": "int n; vector\n\nconst int INF = 1e9;\n\nvoid shortest_paths(int v0, vector\n\nwhile (!q.empty()) {\n    int u = q.front();\n    q.pop_front();\n    m[u] = 0;\n    for (Edge e : adj[u]) {\n        if (d[e.to] > d[u] + e.w) {\n            d[e.to] = d[u] + e.w;\n            p[e.to] = u;\n            if (m[e.to] == 2) {\n                m[e.to] = 1;\n                q.push_back(e.to);\n            } else if (m[e.to] == 0) {\n                m[e.to] = 1;\n                q.push_front(e.to);\n            }\n        }\n    }\n}",
        "score": 1.290268063545227,
        "is_relevant": true
      },
      {
        "text": "tags: - Translated e_maxx_link: rmq_linear\n\nSolve RMQ (Range Minimum Query) by finding LCA (Lowest Common Ancestor)\n\nGiven an array A[0..N-1]. For each query of the form [L, R] we want to find the minimum in the array A starting from position L and ending with position R. We will assume that the array A doesn't change in the process, i.e. this article describes a solution to the static RMQ problem",
        "score": 1.301175832748413,
        "is_relevant": false
      },
      {
        "text": "The algorithm which will be described in this article was developed by Farach-Colton and Bender. It is asymptotically optimal.\n\nAlgorithm\n\nWe use the classical reduction of the LCA problem to the RMQ problem. We traverse all nodes of the tree with DFS and keep an array with all visited nodes and the heights of these nodes. The LCA of two nodes $u$ and $v$ is the node between the occurrences of $u$ and $v$ in the tour, that has the smallest height.",
        "score": 1.331817626953125,
        "is_relevant": false
      },
      {
        "text": "This takes $O(length(S))$ time for preprocessing and then $O(length(ans) \\cdot k)$ for each query (where $ans$ is the answer for the query and $k$ is the size of the alphabet).\n\nSmallest cyclic shift\n\nGiven a string $S$. We want to find the lexicographically smallest cyclic shift.\n\nWe construct a suffix automaton for the string $S + S$. Then the automaton will contain in itself as paths all the cyclic shifts of the string $S$.",
        "score": 1.3328497409820557,
        "is_relevant": false
      },
      {
        "text": "for (int l = 0; l < block_size; l++) {\n        blocks[mask][l][l] = l;\n        for (int r = l + 1; r < block_size; r++) {\n            blocks[mask][l][r] = blocks[mask][l][r - 1];\n            if (b * block_size + r < m)\n                blocks[mask][l][r] = min_by_h(b * block_size + blocks[mask][l][r], \n                        b * block_size + r) - b * block_size;\n        }\n    }\n}",
        "score": 1.3342822790145874,
        "is_relevant": false
      },
      {
        "text": "The answer to the task will be the maximum of all the values $l$.\n\nThe complexity of this part is $O(length(T))$, since in one move we can either increase $l$ by one, or make several passes through the suffix links, each one ends up reducing the value $l$.\n\nImplementation:\n\n```cpp string lcs (string S, string T) { sa_init(); for (int i = 0; i < S.size(); i++) sa_extend(S[i]);",
        "score": 1.3414065837860107,
        "is_relevant": false
      }
    ]
  },
  {
    "query": "Dynamic bridge detection in graphs",
    "chunk_scores": [
      {
        "text": "However in reality it isn't so bad, we can just re-root the smaller of the two trees similar to the ideas in the previous sections, and get $O(\\log n)$ on average.\n\nMore details (including proof of the time complexity) can be found in the article Finding Bridges Online.\n\nHistorical retrospective\n\nThe data structure DSU has been known for a long time.",
        "score": 1.0576844215393066,
        "is_relevant": false
      },
      {
        "text": "Run Floyd-Warshall algorithm on the graph. Initially $d[v][v] = 0$ for each $v$. But after running the algorithm $d[v][v]$ will be smaller than $0$ if there exists a negative length path from $v$ to $v$. We can use this to also find all pairs of vertices that don't have a shortest path between them. We iterate over all pairs of vertices $(i, j)$ and for each pair we check whether they have a shortest path between them. To do this try all possibilities for an intermediate vertex $t$. $(i, j)$ doesn't have a",
        "score": 1.1046497821807861,
        "is_relevant": true
      },
      {
        "text": "Here we can directly apply the data structure, and get a solution that handles an addition of a vertex or an edge and a query in nearly constant time on average.\n\nThis application is quite important, because nearly the same problem appears in Kruskal's algorithm for finding a minimum spanning tree. Using DSU we can improve the $O(m \\log n + n^2)$ complexity to $O(m \\log n)$.\n\nSearch for connected components in an image",
        "score": 1.1270081996917725,
        "is_relevant": true
      },
      {
        "text": "Connected components in a graph\n\nThis is one of the obvious applications of DSU.\n\nFormally the problem is defined in the following way: Initially we have an empty graph. We have to add vertices and undirected edges, and answer queries of the form $(a, b)$ - \"are the vertices $a$ and $b$ in the same connected component of the graph?\"\n\nHere we can directly apply the data structure, and get a solution that handles an addition of a vertex or an edge and a query in nearly constant time on average.",
        "score": 1.1319116353988647,
        "is_relevant": true
      },
      {
        "text": "More details (including proof of the time complexity) can be found in the article Finding Bridges Online.\n\nHistorical retrospective\n\nThe data structure DSU has been known for a long time.\n\nThis way of storing this structure in the form of a forest of trees was apparently first described by Galler and Fisher in 1964 (Galler, Fisher, \"An Improved Equivalence Algorithm), however the complete analysis of the time complexity was conducted much later.",
        "score": 1.1456069946289062,
        "is_relevant": false
      },
      {
        "text": "We can solve this problem by using Depth First Search in $O(M)$ where $M$ is number of edges.\n\nAlgorithm\n\nWe will run a series of DFS in the graph. Initially all vertices are colored white (0). From each unvisited (white) vertex, start the DFS, mark it gray (1) while entering and mark it black (2) on exit. If DFS moves to a gray vertex, then we have found a cycle (if the graph is undirected, the edge to parent is not considered). The cycle itself can be reconstructed using parent array.\n\nImplementation",
        "score": 1.1579102277755737,
        "is_relevant": true
      },
      {
        "text": "Here is an implementation for undirected graph. Note that in the undirected version, if a vertex v gets colored black, it will never be visited again by the DFS. This is because we already explored all connected edges of v when we first visited it. The connected component containing v (after removing the edge between v and its parent) must be a tree, if the DFS has completed processing v without finding a cycle. So we don't even need to distinguish between gray and black states. Thus we can turn the char",
        "score": 1.1746097803115845,
        "is_relevant": true
      },
      {
        "text": "Implementation:\n\nNow we can implement the entire algorithm. First we construct the graph of implications and find all strongly connected components. This can be accomplished with Kosaraju's algorithm in $O(n + m)$ time. In the second traversal of the graph Kosaraju's algorithm visits the strongly connected components in topological order, therefore it is easy to compute $\\text{comp}[v]$ for each vertex $v$.",
        "score": 1.202528476715088,
        "is_relevant": true
      },
      {
        "text": "tags: - Translated e_maxx_link: kirchhoff_theorem\n\nKirchhoff's theorem. Finding the number of spanning trees\n\nProblem: You are given a connected undirected graph (with possible multiple edges) represented using an adjacency matrix. Find the number of different spanning trees of this graph.\n\nThe following formula was proven by Kirchhoff in 1847.\n\nKirchhoff's matrix tree theorem",
        "score": 1.2249343395233154,
        "is_relevant": true
      },
      {
        "text": "```cpp int n; vector\n\nbool dfs(int v, int par) { // passing vertex and its parent vertex visited[v] = true; for (int u : adj[v]) { if(u == par) continue; // skipping edge to parent vertex if (visited[u]) { cycle_end = v; cycle_start = u; return true; } parent[u] = v; if (dfs(u, parent[u])) return true; } return false; }\n\nvoid find_cycle() { visited.assign(n, false); parent.assign(n, -1); cycle_start = -1;\n\nfor (int v = 0; v < n; v++) {\n    if (!visited[v] && dfs(v, parent[v]))\n        break;\n}",
        "score": 1.231001853942871,
        "is_relevant": true
      }
    ]
  },
  {
    "query": "Checking strong connectivity in a graph",
    "chunk_scores": [
      {
        "text": "We will look for the Euler cycle exactly as described above (non-recursive version), and at the same time at the end of this algorithm we will check whether the graph was connected or not (if the graph was not connected, then at the end of the algorithm some edges will remain in the graph, and in this case we need to print $-1$). Finally, the program takes into account that there can be isolated vertices in the graph.",
        "score": 0.8828985095024109,
        "is_relevant": true
      },
      {
        "text": "This criterion can be verified in $O(n + m)$ time by finding all strongly connected components.\n\nThe following image shows all strongly connected components for the example. As we can check easily, neither of the four components contain a vertex $x$ and its negation $\\lnot x$, therefore the example has a solution. We will learn in the next paragraphs how to compute a valid assignment, but just for demonstration purposes the solution $a = \\text{false}$, $b = \\text{false}$, $c = \\text{false}$ is given.",
        "score": 0.9142480492591858,
        "is_relevant": true
      },
      {
        "text": "So we have constructed an algorithm that finds the required values of variables under the assumption that for any variable $x$ the vertices $x$ and $\\lnot x$ are in different strongly connected components. Above showed the correctness of this algorithm. Consequently we simultaneously proved the above criterion for the existence of a solution.\n\nImplementation:",
        "score": 0.9162775874137878,
        "is_relevant": true
      },
      {
        "text": "Connected components in a graph\n\nThis is one of the obvious applications of DSU.\n\nFormally the problem is defined in the following way: Initially we have an empty graph. We have to add vertices and undirected edges, and answer queries of the form $(a, b)$ - \"are the vertices $a$ and $b$ in the same connected component of the graph?\"\n\nHere we can directly apply the data structure, and get a solution that handles an addition of a vertex or an edge and a query in nearly constant time on average.",
        "score": 0.9385033249855042,
        "is_relevant": true
      },
      {
        "text": "Let us sort the strongly connected components in topological order (i.e. $\\text{comp}[v] \\le \\text{comp}[u]$ if there is a path from $v$ to $u$) and let $\\text{comp}[v]$ denote the index of strongly connected component to which the vertex $v$ belongs. Then, if $\\text{comp}[x] < \\text{comp}[\\lnot x]$ we assign $x$ with $\\text{false}$ and $\\text{true}$ otherwise.",
        "score": 0.9453678131103516,
        "is_relevant": true
      },
      {
        "text": "In order for this 2-SAT problem to have a solution, it is necessary and sufficient that for any variable $x$ the vertices $x$ and $\\lnot x$ are in different strongly connected components of the strong connection of the implication graph.\n\nThis criterion can be verified in $O(n + m)$ time by finding all strongly connected components.",
        "score": 0.9678007364273071,
        "is_relevant": true
      },
      {
        "text": "be $\\text{true}$ and visa versa. It turns out, that this condition is not only necessary, but also sufficient. We will prove this in a few paragraphs below. First recall, if a vertex is reachable from a second one, and the second one is reachable from the first one, then these two vertices are in the same strongly connected component. Therefore we can formulate the criterion for the existence of a solution as follows:",
        "score": 0.969965398311615,
        "is_relevant": true
      },
      {
        "text": "Implementation:\n\nNow we can implement the entire algorithm. First we construct the graph of implications and find all strongly connected components. This can be accomplished with Kosaraju's algorithm in $O(n + m)$ time. In the second traversal of the graph Kosaraju's algorithm visits the strongly connected components in topological order, therefore it is easy to compute $\\text{comp}[v]$ for each vertex $v$.",
        "score": 0.9732573628425598,
        "is_relevant": true
      },
      {
        "text": "First we prove that the vertex $x$ cannot reach the vertex $\\lnot x$. Because we assigned $\\text{true}$ it has to hold that the index of strongly connected component of $x$ is greater than the index of the component of $\\lnot x$. This means that $\\lnot x$ is located on the left of the component containing $x$, and the later vertex cannot reach the first.",
        "score": 1.0033438205718994,
        "is_relevant": true
      },
      {
        "text": "Algorithm\n\nFirst we can check if there is an Eulerian path. We can use the following theorem. An Eulerian cycle exists if and only if the degrees of all vertices are even. And an Eulerian path exists if and only if the number of vertices with odd degrees is two (or zero, in the case of the existence of a Eulerian cycle). In addition, of course, the graph must be sufficiently connected (i.e., if you remove all isolated vertices from it, you should get a connected graph).",
        "score": 1.0637917518615723,
        "is_relevant": true
      }
    ]
  },
  {
    "query": "Randomized heap implementation",
    "chunk_scores": [
      {
        "text": "Algorithm\n\nWe construct a Cartesian tree from the array A. A Cartesian tree of an array A is a binary tree with the min-heap property (the value of parent node has to be smaller or equal than the value of its children) such that the in-order traversal of the tree visits the nodes in the same order as they are in the array A.",
        "score": 1.0439238548278809,
        "is_relevant": true
      },
      {
        "text": "You can find a proof of the complexity and even more union techniques here.\n\n```cpp void make_set(int v) { parent[v] = v; index[v] = rand(); }\n\nvoid union_sets(int a, int b) { a = find_set(a); b = find_set(b); if (a != b) { if (index[a] < index[b]) swap(a, b); parent[b] = a; } } ```",
        "score": 1.209615707397461,
        "is_relevant": false
      },
      {
        "text": "Using such implementation the algorithm consumes $O(n)$ of the memory (obviously) and performs $O(n \\log \\log n)$ (see next section).\n\nAsymptotic analysis",
        "score": 1.2134478092193604,
        "is_relevant": false
      },
      {
        "text": "therefor also from l' and r'. This automatically follows from the min-heap property. And is also has to be the lowest ancestor, because otherwise l' and r' would be both in the left or in the right subtree, which generates a contradiction since in such a case the minimum wouldn't even be in the range.",
        "score": 1.233048915863037,
        "is_relevant": true
      },
      {
        "text": "If $v$ belongs to $M_0$, and if $d_v$ can be improved $d_v > d_u + w$, then we improve $d_v$ and insert the vertex $v$ back to the set $M_1$, placing it at the beginning of the queue.\n\nAnd of course, with each update in the array $d$ we also have to update the corresponding element in the array $p$.\n\nImplementation\n\nWe will use an array $m$ to store in which set each vertex is currently.\n\n```{.cpp file=desopo_pape} struct Edge { int to, w; };\n\nint n; vector\n\nconst int INF = 1e9;",
        "score": 1.2549442052841187,
        "is_relevant": false
      },
      {
        "text": "if (may_be_unary && is_unary(cur_op))\n            cur_op = -cur_op;\n        while (!op.empty() && (\n                (cur_op >= 0 && priority(op.top()) >= priority(cur_op)) ||\n                (cur_op < 0 && priority(op.top()) > priority(cur_op))\n            )) {\n            process_op(st, op.top());\n            op.pop();\n        }\n        op.push(cur_op);\n        may_be_unary = true;\n    } else {\n        int number = 0;\n        while (i < (int)s.size() && isalnum(s[i]))",
        "score": 1.260703444480896,
        "is_relevant": false
      },
      {
        "text": "cpp void union_sets(int a, int b) { a = find_set(a); b = find_set(b); if (a != b) { if (rand() % 2) swap(a, b); parent[b] = a; } }\n\nApplications and various improvements\n\nIn this section we consider several applications of the data structure, both the trivial uses and some improvements to the data structure.\n\nConnected components in a graph\n\nThis is one of the obvious applications of DSU.",
        "score": 1.2695889472961426,
        "is_relevant": false
      },
      {
        "text": "the second pointer will come around to meet the first one during the loops. If the cycle length is $\\lambda$ and the $\\mu$ is the first index at which the cycle starts, then the algorithm will run in $O(\\lambda + \\mu)$ time.",
        "score": 1.2962303161621094,
        "is_relevant": false
      },
      {
        "text": "This algorithm finds a cycle by using two pointers moving over the sequence at differing speeds. During each iteration, the first pointer will advance one element over, while the second pointer advances to every other element. Using this idea it is easy to observe that if there is a cycle, at some point the second pointer will come around to meet the first one during the loops. If the cycle length is $\\lambda$ and the $\\mu$ is the first index at which the cycle starts, then the algorithm will run in",
        "score": 1.29676353931427,
        "is_relevant": false
      },
      {
        "text": "This algorithm is also known as the Tortoise and Hare algorithm, based on the tale in which a tortoise (the slow pointer) and a hare (the faster pointer) have a race.",
        "score": 1.2976131439208984,
        "is_relevant": false
      }
    ]
  },
  {
    "query": "Heavy-Light Decomposition (HLD)",
    "chunk_scores": [
      {
        "text": "tags: - Translated e_maxx_link: fft_multiply\n\nFast Fourier transform\n\nIn this article we will discuss an algorithm that allows us to multiply two polynomials of length $n$ in $O(n \\log n)$ time, which is better than the trivial multiplication which takes $O(n^2)$ time. Obviously also multiplying two long numbers can be reduced to multiplying polynomials, so also two long numbers can be multiplied in $O(n \\log n)$ time (where $n$ is the number of digits in the numbers).",
        "score": 1.4893872737884521,
        "is_relevant": false
      },
      {
        "text": "Thus the computation of the inverse DFT is almost the same as the calculation of the direct DFT, and it also can be performed in $O(n \\log n)$ time.\n\nImplementation\n\nHere we present a simple recursive implementation of the FFT and the inverse FFT, both in one function, since the difference between the forward and the inverse FFT are so minimal. To store the complex numbers we use the complex type in the C++ STL.\n\n```{.cpp file=fft_recursive} using cd = complex\n\nvoid fft(vector",
        "score": 1.5071823596954346,
        "is_relevant": false
      },
      {
        "text": "This approach therefore requires computing the products of polynomials with smaller coefficients (by using the normal FFT and inverse FFT), and then the original product can be restored using modular addition and multiplication in $O(n)$ time.\n\nApplications\n\nDFT can be used in a huge variety of other problems, which at the first glance have nothing to do with multiplying polynomials.\n\nAll possible sums",
        "score": 1.508226752281189,
        "is_relevant": false
      },
      {
        "text": "To apply it in the fast Fourier transform algorithm, we need a root to exist for some $n$, which is a power of $2$, and also for all smaller powers. We can notice the following interesting property:\n\n$$\\begin{align} (w_n^2)^m = w_n^n &= 1 \\pmod{p}, \\quad \\text{with } m = \\frac{n}{2}\\ (w_n^2)^k = w_n^{2k} &\\ne 1 \\pmod{p}, \\quad 1 \\le k < m. \\end{align}$$",
        "score": 1.511960744857788,
        "is_relevant": false
      },
      {
        "text": "Effective computation of large exponents modulo a number\n\nProblem: Compute $x^n \\bmod m$. This is a very common operation. For instance it is used in computing the modular multiplicative inverse.\n\nSolution: Since we know that the modulo operator doesn't interfere with multiplications ($a \\cdot b \\equiv (a \\bmod m) \\cdot (b \\bmod m) \\pmod m$), we can directly use the same code, and just replace every multiplication with a modular multiplication:",
        "score": 1.5311639308929443,
        "is_relevant": false
      },
      {
        "text": "As we see, we can perform the Montgomery reduction without any heavy modulo operations. If we choose $r$ as a power of $2$, the modulo operations and divisions in the algorithm can be computed using bitmasking and shifting.\n\nA second application of the Montgomery reduction is to transfer a number back from the Montgomery space into the normal space.\n\nFast inverse trick",
        "score": 1.5326100587844849,
        "is_relevant": false
      },
      {
        "text": "```{.cpp file=fft_implementation_modular_arithmetic} const int mod = 7340033; const int root = 5; const int root_1 = 4404020; const int root_pw = 1 << 20;\n\nvoid fft(vector\n\nfor (int i = 1, j = 0; i < n; i++) {\n    int bit = n >> 1;\n    for (; j & bit; bit >>= 1)\n        j ^= bit;\n    j ^= bit;\n\n    if (i < j)\n        swap(a[i], a[j]);\n}\n\nfor (int len = 2; len <= n; len <<= 1) {\n    int wlen = invert ? root_1 : root;\n    for (int i = len; i < root_pw; i <<= 1)\n        wlen = (int)(1LL * wlen * wlen % mod);",
        "score": 1.5371854305267334,
        "is_relevant": false
      },
      {
        "text": "Thus we obtain the formula:\n\n$$a_k = \\frac{1}{n} \\sum_{j=0}^{n-1} y_j w_n^{-k j}$$\n\nComparing this to the formula for $y_k$\n\n$$y_k = \\sum_{j=0}^{n-1} a_j w_n^{k j},$$\n\nwe notice that these problems are almost the same, so the coefficients $a_k$ can be found by the same divide and conquer algorithm, as well as the direct FFT, only instead of $w_n^k$ we have to use $w_n^{-k}$, and at the end we need to divide the resulting coefficients by $n$.",
        "score": 1.5398366451263428,
        "is_relevant": false
      },
      {
        "text": "Thus we computed the required DFT from the vector $a$.\n\nHere we described the process of computing the DFT only at the first recursion level, but the same works obviously also for all other levels. Thus, after applying the bit-reversal permutation, we can compute the DFT in-place, without any additional memory.",
        "score": 1.5427899360656738,
        "is_relevant": false
      },
      {
        "text": "cpp long long binpow(long long a, long long b) { long long res = 1; while (b > 0) { if (b & 1) res = res * a; a = a * a; b >>= 1; } return res; }\n\nApplications\n\nEffective computation of large exponents modulo a number\n\nProblem: Compute $x^n \\bmod m$. This is a very common operation. For instance it is used in computing the modular multiplicative inverse.",
        "score": 1.5439586639404297,
        "is_relevant": false
      }
    ]
  },
  {
    "query": "Determinant calculation using Gaussian elimination",
    "chunk_scores": [
      {
        "text": "This matrix is called the Vandermonde matrix.\n\nThus we can compute the vector $(a_0, a_1, \\dots, a_{n-1})$ by multiplying the vector $(y_0, y_1, \\dots y_{n-1})$ from the left with the inverse of the matrix:",
        "score": 1.314504861831665,
        "is_relevant": false
      },
      {
        "text": "Using the solution for the one-dimensional case, it is easy to obtain a solution in $O(n^3)$ for the two-dimensions case: we iterate over all possible values of $l_1$ and $r_1$, and calculate the sums from $l_1$ to $r_1$ in each row of the matrix. Now we have the one-dimensional problem of finding the indices $l_2$ and $r_2$ in this array, which can already be solved in linear time.",
        "score": 1.3821706771850586,
        "is_relevant": false
      },
      {
        "text": "The determinant of the matrix can be found in $O(N^3)$ by using the Gaussian method.\n\nThe proof of this theorem is quite difficult and is not presented here; for an outline of the proof and variations of the theorem for graphs without multiple edges and for directed graphs refer to Wikipedia.\n\nRelation to Kirchhoff's circuit laws",
        "score": 1.4044079780578613,
        "is_relevant": true
      },
      {
        "text": "that, when multiplied by a vector with the old coordinates and an unit gives a new vector with the new coordinates and an unit:\n\n$$\\begin{pmatrix} x & y & z & 1 \\end{pmatrix} \\cdot \\begin{pmatrix} a_{11} & a_ {12} & a_ {13} & a_ {14} \\ a_{21} & a_ {22} & a_ {23} & a_ {24} \\ a_{31} & a_ {32} & a_ {33} & a_ {34} \\ a_{41} & a_ {42} & a_ {43} & a_ {44} \\end{pmatrix} = \\begin{pmatrix} x' & y' & z' & 1 \\end{pmatrix}$$",
        "score": 1.4247026443481445,
        "is_relevant": false
      },
      {
        "text": "```cpp int main() { int n; vector\n\nvector<int> deg(n);\nfor (int i = 0; i < n; ++i) {\n    for (int j = 0; j < n; ++j)\n        deg[i] += g[i][j];\n}\n\nint first = 0;\nwhile (first < n && !deg[first])\n    ++first;\nif (first == n) {\n    cout << -1;\n    return 0;\n}\n\nint v1 = -1, v2 = -1;\nbool bad = false;\nfor (int i = 0; i < n; ++i) {\n    if (deg[i] & 1) {\n        if (v1 == -1)\n            v1 = i;\n        else if (v2 == -1)\n            v2 = i;\n        else\n            bad = true;\n    }\n}",
        "score": 1.4475486278533936,
        "is_relevant": false
      },
      {
        "text": "& \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\ w_n^0 & w_n^{n-1} & w_n^{2(n-1)} & w_n^{3(n-1)} & \\cdots & w_n^{(n-1)(n-1)} \\end{pmatrix}^{-1} \\begin{pmatrix} y_0 \\ y_1 \\ y_2 \\ y_3 \\ \\vdots \\ y_{n-1} \\end{pmatrix} $$",
        "score": 1.4499852657318115,
        "is_relevant": false
      },
      {
        "text": "the matrix that you get after removing the $i$-th row and $j$-th column. So you can, for example, delete the last row and last column of the matrix $L$, and the absolute value of the determinant of the resulting matrix will give you the number of spanning trees.",
        "score": 1.4537253379821777,
        "is_relevant": false
      },
      {
        "text": "$$ \\newcommand\\T{\\Rule{0pt}{1em}{.3em}} \\begin{array}{|l|l|l|l|l|l|} \\hline i & x_i \\bmod n & x_{2i} \\bmod n & x_i \\bmod 317 & x_{2i} \\bmod 317 & \\gcd(x_i - x_{2i}, n) \\ \\hline 0 & 2 & 2 & 2 & 2 & - \\ 1 & 5 & 26 & 5 & 26 & 1 \\ 2 & 26 & 458330 & 26 & 265 & 1 \\ 3 & 677 & 1671573 & 43 & 32 & 1 \\ 4 & 458330 & 641379 & 265 & 88 & 1 \\ 5 & 1166412 & 351937 & 169 & 67 & 1 \\ 6 & 1671573 & 1264682 & 32 & 169 & 1 \\ 7 & 2193080 & 2088470 & 74 & 74 & 317 \\ \\hline \\end{array}$$",
        "score": 1.471067190170288,
        "is_relevant": false
      },
      {
        "text": "& w_n^{2(n-1)} & w_n^{3(n-1)} & \\cdots & w_n^{(n-1)(n-1)} \\end{pmatrix} \\begin{pmatrix} a_0 \\ a_1 \\ a_2 \\ a_3 \\ \\vdots \\ a_{n-1} \\end{pmatrix} = \\begin{pmatrix} y_0 \\ y_1 \\ y_2 \\ y_3 \\ \\vdots \\ y_{n-1} \\end{pmatrix} $$",
        "score": 1.4882276058197021,
        "is_relevant": false
      },
      {
        "text": "A quick check can verify that the inverse of the matrix has the following form:\n\n$$ \\frac{1}{n} \\begin{pmatrix} w_n^0 & w_n^0 & w_n^0 & w_n^0 & \\cdots & w_n^0 \\ w_n^0 & w_n^{-1} & w_n^{-2} & w_n^{-3} & \\cdots & w_n^{-(n-1)} \\ w_n^0 & w_n^{-2} & w_n^{-4} & w_n^{-6} & \\cdots & w_n^{-2(n-1)} \\ w_n^0 & w_n^{-3} & w_n^{-6} & w_n^{-9} & \\cdots & w_n^{-3(n-1)} \\ \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\ w_n^0 & w_n^{-(n-1)} & w_n^{-2(n-1)} & w_n^{-3(n-1)} & \\cdots & w_n^{-(n-1)(n-1)} \\end{pmatrix} $$",
        "score": 1.4886469841003418,
        "is_relevant": false
      }
    ]
  },
  {
    "query": "Efficient computation of all divisors",
    "chunk_scores": [
      {
        "text": "tags: - Original\n\nInteger factorization\n\nIn this article we list several algorithms for the factorization of integers, each of which can be either fast or varying levels of slow depending on their input.",
        "score": 0.8660810589790344,
        "is_relevant": true
      },
      {
        "text": "Trial division\n\nThis is the most basic algorithm to find a prime factorization.\n\nWe divide by each possible divisor $d$. It can be observed that it is impossible for all prime factors of a composite number $n$ to be bigger than $\\sqrt{n}$. Therefore, we only need to test the divisors $2 \\le d \\le \\sqrt{n}$, which gives us the prime factorization in $O(\\sqrt{n})$. (This is pseudo-polynomial time, i.e. polynomial in the value of the input but exponential in the number of bits of the input.)",
        "score": 0.8808310031890869,
        "is_relevant": true
      },
      {
        "text": "} ```\n\nThe combination of a trial division for small prime numbers together with Brent's version of Pollard's rho algorithm makes a very powerful factorization algorithm.\n\nPractice Problems\n\nSPOJ - FACT0\n\nSPOJ - FACT1\n\nSPOJ - FACT2\n\nGCPC 15 - Divisions",
        "score": 0.930793046951294,
        "is_relevant": true
      },
      {
        "text": "```cpp int gcd(int a, int b) { return a ? gcd(b % a, a) : b; }\n\nint powmod(int a, int b, int p) { int res = 1; while (b > 0) { if (b & 1) { res = res * a % p; } a = a * a % p; b >>= 1; } return res; }\n\n// Finds the primitive root modulo p int generator(int p) { vector\n\nfor (int res = 2; res <= p; ++res) {\n    bool ok = true;\n    for (int factor : fact) {\n        if (powmod(res, phi / factor, p) == 1) {\n            ok = false;\n            break;\n        }\n    }\n    if (ok) return res;\n}\nreturn -1;",
        "score": 0.9420623779296875,
        "is_relevant": false
      },
      {
        "text": "// compute a^M\n    for (int p : primes) {\n        if (p >= B)\n            continue;\n        long long p_power = 1;\n        while (p_power * p <= B)\n            p_power *= p;\n        a = power(a, p_power, n);\n\n        g = gcd(a - 1, n);\n        if (g > 1 && g < n)\n            return g;\n    }\n    B *= 2;\n}\nreturn 1;\n\n```\n\nObserve that this is a probabilistic algorithm. A consequence of this is that there is a possibility of the algorithm being unable to find a factor at all.",
        "score": 0.9472478628158569,
        "is_relevant": false
      },
      {
        "text": "Effective computation of large exponents modulo a number\n\nProblem: Compute $x^n \\bmod m$. This is a very common operation. For instance it is used in computing the modular multiplicative inverse.\n\nSolution: Since we know that the modulo operator doesn't interfere with multiplications ($a \\cdot b \\equiv (a \\bmod m) \\cdot (b \\bmod m) \\pmod m$), we can directly use the same code, and just replace every multiplication with a modular multiplication:",
        "score": 0.9566769599914551,
        "is_relevant": false
      },
      {
        "text": "Long Integer Arithmetic for Factorization Representation\n\nThe idea is to store the integer as its factorization, i.e. the powers of primes which divide it.\n\nThis approach is very easy to implement, and allows to do multiplication and division easily (asymptotically faster than the classical method), but not addition or subtraction. It is also very memory-efficient compared to the classical approach.",
        "score": 0.9620828032493591,
        "is_relevant": true
      },
      {
        "text": "The smallest divisor must be a prime number. We remove the factored number, and continue the process. If we cannot find any divisor in the range $[2; \\sqrt{n}]$, then the number itself has to be prime.\n\n{.cpp file=factorization_trial_division1} vector<long long> trial_division1(long long n) { vector<long long> factorization; for (long long d = 2; d * d <= n; d++) { while (n % d == 0) { factorization.push_back(d); n /= d; } } if (n > 1) factorization.push_back(n); return factorization; }",
        "score": 0.9623165130615234,
        "is_relevant": true
      },
      {
        "text": "Additional optimization: If runtime is extremely important, you can try to replace two divisions with one by finding only integer result of division (variable carry) and then use it to find modulo using multiplication. This usually makes the code faster, though not dramatically.\n\nMultiplication by long integer\n\nMultiply long integers $a$ and $b$ and store result in $c$:",
        "score": 0.9932801723480225,
        "is_relevant": false
      },
      {
        "text": "Obviously, the complexity is worse, which is $O((R - L + 1) \\log (R) + \\sqrt R)$. However, it still runs very fast in practice.\n\nLinear time modification\n\nWe can modify the algorithm in a such a way, that it only has linear time complexity. This approach is described in the article Linear Sieve. However, this algorithm also has its own weaknesses.\n\nPractice Problems\n\nLeetcode - Four Divisors\n\nLeetcode - Count Primes\n\nSPOJ - Printing Some Primes\n\nSPOJ - A Conjecture of Paul Erdos\n\nSPOJ - Primal Fear",
        "score": 1.0051324367523193,
        "is_relevant": true
      }
    ]
  },
  {
    "query": "Convex optimization in dynamic programming",
    "chunk_scores": [
      {
        "text": "return dp_before[n - 1];\n\n} ```\n\nThings to look out for\n\nThe greatest difficulty with Divide and Conquer DP problems is proving the monotonicity of $opt$. One special case where this is true is when the cost function satisfies the quadrangle inequality, i.e., $C(a, c) + C(b, d) \\leq C(a, d) + C(b, c)$ for all $a \\leq b \\leq c \\leq d$. Many Divide and Conquer DP problems can also be solved with the Convex Hull trick or vice-versa. It is useful to know and understand both!\n\nPractice Problems",
        "score": 1.0098425149917603,
        "is_relevant": true
      },
      {
        "text": "tags: - Original\n\nDivide and Conquer DP\n\nDivide and Conquer is a dynamic programming optimization.\n\nPreconditions\n\nSome dynamic programming problems have a recurrence of this form:\n\n$$ dp(i, j) = \\min_{0 \\leq k \\leq j} \\{ dp(i - 1, k - 1) + C(k, j) \\} $$\n\nwhere $C(k, j)$ is a cost function and $dp(i, j) = 0$ when $j \\lt 0$.",
        "score": 1.117319107055664,
        "is_relevant": true
      },
      {
        "text": "Let $opt(i, j)$ be the value of $k$ that minimizes the above expression. Assuming that the cost function satisfies the quadrangle inequality, we can show that $opt(i, j) \\leq opt(i, j + 1)$ for all $i, j$. This is known as the monotonicity condition. Then, we can apply divide and conquer DP. The optimal \"splitting point\" for a fixed $i$ increases as $j$ increases.",
        "score": 1.13783860206604,
        "is_relevant": true
      },
      {
        "text": "It is easy to see, that if the schedule $\\pi$ is optimal, than any change in it leads to an increased penalty (or to the identical penalty), therefore for the optimal schedule we can write down the following condition:\n\n$$c_{\\pi_{i}} \\cdot t_{\\pi_{i+1}} - c_{\\pi_{i+1}} \\cdot t_{\\pi_i} \\ge 0 \\quad \\forall i = 1 \\dots n-1$$\n\nAnd after rearranging we get:\n\n$$\\frac{c_{\\pi_i}}{t_{\\pi_i}} \\ge \\frac{c_{\\pi_{i+1}}}{t_{\\pi_{i+1}}} \\quad \\forall i = 1 \\dots n-1$$",
        "score": 1.2061229944229126,
        "is_relevant": false
      },
      {
        "text": "Identical case: $f_i(t) = \\phi(t)$, where $\\phi$ is a monotone increasing function.\n\nIn all other cases the method cannot be applied.\n\nThe theorem is proven under the assumption that the penalty functions are sufficiently smooth (the third derivatives exists).\n\nIn all three case we apply the permutation method, through which the desired optimal schedule can be found by sorting, hence in $O(n \\log n)$ time.",
        "score": 1.221940040588379,
        "is_relevant": false
      },
      {
        "text": "Solutions for special cases\n\nLinear penalty functions\n\nFirst we will solve the problem in the case that all penalty functions $f_i(t)$ are linear, i.e. they have the form $f_i(t) = c_i \\cdot t$, where $c_i$ is a non-negative number. Note that these functions don't have a constant term. Otherwise we can sum up all constant term, and resolve the problem without them.",
        "score": 1.227691888809204,
        "is_relevant": false
      },
      {
        "text": "If $v$ belongs to $M_2$, then $v$ is inserted into the set $M_1$ by inserting it at the back of the queue. $d_v$ is set to $d_u + w$.\n\nIf $v$ belongs to $M_1$, then we try to improve the value of $d_v$: $d_v = \\min(d_v, d_u + w)$. Since $v$ is already in $M_1$, we don't need to insert it into $M_1$ and the queue.\n\nIf $v$ belongs to $M_0$, and if $d_v$ can be improved $d_v > d_u + w$, then we improve $d_v$ and insert the vertex $v$ back to the set $M_1$, placing it at the beginning of the queue.",
        "score": 1.240308403968811,
        "is_relevant": false
      },
      {
        "text": "To minimize the runtime, we apply the idea behind divide and conquer. First, compute $opt(i, n / 2)$. Then, compute $opt(i, n / 4)$, knowing that it is less than or equal to $opt(i, n / 2)$ and $opt(i, 3 n / 4)$ knowing that it is greater than or equal to $opt(i, n / 2)$. By recursively keeping track of the lower and upper bounds on $opt$, we reach a $O(m n \\log n)$ runtime. Each possible value of $opt(i, j)$ only appears in $\\log n$ different nodes.",
        "score": 1.2411599159240723,
        "is_relevant": true
      },
      {
        "text": "We first give a solution that finds a simple numerical answer without finding the indices of the desired segment:\n\n```cpp int ans = a[0], sum = 0, min_sum = 0;\n\nfor (int r = 0; r < n; ++r) { sum += a[r]; ans = max(ans, sum - min_sum); min_sum = min(min_sum, sum); } ```\n\nNow we give a full version of the solution, which additionally also finds the boundaries of the desired segment:\n\n```cpp int ans = a[0], ans_l = 0, ans_r = 0; int sum = 0, min_sum = 0, min_pos = -1;",
        "score": 1.2528783082962036,
        "is_relevant": false
      },
      {
        "text": "On the other hand this method has also some advantages: with this method you don't have to think about any tricky properties in the dynamic programming solution. And this approach allows us to generalize the problem very easily (see below).\n\nRelated tasks\n\nHere are several problems that are closely related to the problem of finding the longest increasing subsequence.\n\nLongest non-decreasing subsequence",
        "score": 1.2612943649291992,
        "is_relevant": false
      }
    ]
  },
  {
    "query": "Segment intersection in 2D geometry",
    "chunk_scores": [
      {
        "text": "Fast application of a set of geometric operations to a set of points",
        "score": 1.120741605758667,
        "is_relevant": false
      },
      {
        "text": "However, this is not enough to prove the algorithm. In the algorithm, we are actually limited in finding the answer only to such segments that begin immediately after the places when $s<0$ happened.",
        "score": 1.2123639583587646,
        "is_relevant": false
      },
      {
        "text": "void dfs2(int v, int cl) {\n    comp[v] = cl;\n    for (int u : adj_t[v]) {\n        if (comp[u] == -1)\n            dfs2(u, cl);\n    }\n}\n\nbool solve_2SAT() {\n    order.clear();\n    used.assign(n_vertices, false);\n    for (int i = 0; i < n_vertices; ++i) {\n        if (!used[i])\n            dfs1(i);\n    }\n\n    comp.assign(n_vertices, -1);\n    for (int i = 0, j = 0; i < n_vertices; ++i) {\n        int v = order[n_vertices - i - 1];\n        if (comp[v] == -1)\n            dfs2(v, j++);\n    }",
        "score": 1.2340126037597656,
        "is_relevant": false
      },
      {
        "text": "As in algorithm 1, we first gave a simplified implementation that looks for only a numerical answer without finding the boundaries of the desired segment:\n\n```cpp int ans = a[0], sum = 0;\n\nfor (int r = 0; r < n; ++r) { sum += a[r]; ans = max(ans, sum); sum = max(sum, 0); } ```\n\nA complete solution, maintaining the indexes of the boundaries of the corresponding segment:\n\n```cpp int ans = a[0], ans_l = 0, ans_r = 0; int sum = 0, minus_pos = -1;",
        "score": 1.2512367963790894,
        "is_relevant": false
      },
      {
        "text": "We first give a solution that finds a simple numerical answer without finding the indices of the desired segment:\n\n```cpp int ans = a[0], sum = 0, min_sum = 0;\n\nfor (int r = 0; r < n; ++r) { sum += a[r]; ans = max(ans, sum - min_sum); min_sum = min(min_sum, sum); } ```\n\nNow we give a full version of the solution, which additionally also finds the boundaries of the desired segment:\n\n```cpp int ans = a[0], ans_l = 0, ans_r = 0; int sum = 0, min_sum = 0, min_pos = -1;",
        "score": 1.2583725452423096,
        "is_relevant": false
      },
      {
        "text": "Now we give a full version of the solution, which additionally also finds the boundaries of the desired segment:\n\n```cpp int ans = a[0], ans_l = 0, ans_r = 0; int sum = 0, min_sum = 0, min_pos = -1;\n\nfor (int r = 0; r < n; ++r) { sum += a[r]; int cur = sum - min_sum; if (cur > ans) { ans = cur; ans_l = min_pos + 1; ans_r = r; } if (sum < min_sum) { min_sum = sum; min_pos = r; } } ```\n\nAlgorithm 2",
        "score": 1.2781078815460205,
        "is_relevant": false
      },
      {
        "text": "comp.assign(n_vertices, -1);\n    for (int i = 0, j = 0; i < n_vertices; ++i) {\n        int v = order[n_vertices - i - 1];\n        if (comp[v] == -1)\n            dfs2(v, j++);\n    }\n\n    assignment.assign(n_vars, false);\n    for (int i = 0; i < n_vertices; i += 2) {\n        if (comp[i] == comp[i + 1])\n            return false;\n        assignment[i / 2] = comp[i] > comp[i + 1];\n    }\n    return true;\n}",
        "score": 1.305328607559204,
        "is_relevant": false
      },
      {
        "text": "```cpp void make_set(int v) { parent[v] = make_pair(v, 0); rank[v] = 0; bipartite[v] = true; }\n\npair\n\nvoid add_edge(int a, int b) { pair\n\npair<int, int> pb = find_set(b);\nb = pb.first;\nint y = pb.second;\n\nif (a == b) {\n    if (x == y)\n        bipartite[a] = false;\n} else {\n    if (rank[a] < rank[b])\n        swap (a, b);\n    parent[b] = make_pair(a, x^y^1);\n    bipartite[a] &= bipartite[b];\n    if (rank[a] == rank[b])\n        ++rank[a];\n}",
        "score": 1.3228789567947388,
        "is_relevant": false
      },
      {
        "text": "Thus, as an initial approximation for $z[i]$ we can safely take:\n\n$$ z_0[i] = \\min(r - i,\\; z[i-l]) $$\n\nAfter having $z[i]$ initialized to $z_0[i]$, we try to increment $z[i]$ by running the trivial algorithm -- because in general, after the border $r$, we cannot know if the segment will continue to match or not.",
        "score": 1.3270330429077148,
        "is_relevant": false
      },
      {
        "text": "A complete solution, maintaining the indexes of the boundaries of the corresponding segment:\n\n```cpp int ans = a[0], ans_l = 0, ans_r = 0; int sum = 0, minus_pos = -1;\n\nfor (int r = 0; r < n; ++r) { sum += a[r]; if (sum > ans) { ans = sum; ans_l = minus_pos + 1; ans_r = r; } if (sum < 0) { sum = 0; minus_pos = r; } } ```\n\nRelated tasks\n\nFinding the maximum/minimum subarray with constraints",
        "score": 1.340859055519104,
        "is_relevant": true
      }
    ]
  },
  {
    "query": "Modular square root computation",
    "chunk_scores": [
      {
        "text": "Effective computation of large exponents modulo a number\n\nProblem: Compute $x^n \\bmod m$. This is a very common operation. For instance it is used in computing the modular multiplicative inverse.\n\nSolution: Since we know that the modulo operator doesn't interfere with multiplications ($a \\cdot b \\equiv (a \\bmod m) \\cdot (b \\bmod m) \\pmod m$), we can directly use the same code, and just replace every multiplication with a modular multiplication:",
        "score": 0.8963849544525146,
        "is_relevant": false
      },
      {
        "text": "if (invert) {\n    int n_1 = inverse(n, mod);\n    for (int & x : a)\n        x = (int)(1LL * x * n_1 % mod);\n}\n\n} ```\n\nHere the function inverse computes the modular inverse (see Modular Multiplicative Inverse). The constants mod, root, root_pw determine the module and the root, and root_1 is the inverse of root modulo mod.",
        "score": 0.9248862862586975,
        "is_relevant": true
      },
      {
        "text": "In practice this implementation is slower than the implementation using complex numbers (due to the huge number of modulo operations), but it has some advantages such as less memory usage and no rounding errors.\n\nMultiplication with arbitrary modulus",
        "score": 0.9755662679672241,
        "is_relevant": false
      },
      {
        "text": "This gives us the following algorithm to compute $x \\cdot r^{-1} \\bmod n$:\n\ntext function reduce(x): q = (x mod r) * n' mod r a = (x - q * n) / r if a < 0: a += n return a\n\nSince $x < n \\cdot n < r \\cdot n$ (even if $x$ is the product of a multiplication) and $q \\cdot n < r \\cdot n$ we know that $-n < (x - q \\cdot n) / r < n$. Therefore the final modulo operation is implemented using a single check and one addition.",
        "score": 0.9816117286682129,
        "is_relevant": false
      },
      {
        "text": "This means we can start with $x = 1$ as the inverse of $a$ modulo $2^1$, apply the trick a few times and in each iteration we double the number of correct bits of $x$.\n\nImplementation\n\nUsing the GCC compiler we can compute $x \\cdot y \\bmod n$ still efficiently, when all three numbers are 64 bit integer, since the compiler supports 128 bit integer with the types __int128 and __uint128.\n\ncpp long long result = (__int128)x * y % n;",
        "score": 0.9867885112762451,
        "is_relevant": true
      },
      {
        "text": "You can notice the following relation:\n\n$$\\bar{x} := x \\cdot r \\bmod n = x \\cdot r^2 / r = x * r^2$$\n\nTransforming a number into the space is just a multiplication inside the space of the number with $r^2$. Therefore we can precompute $r^2 \\bmod n$ and just perform a multiplication instead of shifting the number 128 times.",
        "score": 1.0107038021087646,
        "is_relevant": false
      },
      {
        "text": "Our main task here is to compute $x$ such that $g^x \\equiv y \\pmod{2^d}$, where $g=5$ and $y$ is a number of kind $2^n+1$.\n\nSquaring both parts $k$ times we arrive to\n\n$$ g^{2^k x} \\equiv y^{2^k} \\pmod{2^d}. $$",
        "score": 1.0122570991516113,
        "is_relevant": false
      },
      {
        "text": "cpp long long binpow(long long a, long long b) { long long res = 1; while (b > 0) { if (b & 1) res = res * a; a = a * a; b >>= 1; } return res; }\n\nApplications\n\nEffective computation of large exponents modulo a number\n\nProblem: Compute $x^n \\bmod m$. This is a very common operation. For instance it is used in computing the modular multiplicative inverse.",
        "score": 1.0214115381240845,
        "is_relevant": false
      },
      {
        "text": "$$(X \\cdot Y) \\cdot Z = X \\cdot (Y \\cdot Z)$$\n\nMost obviously this applies to modular multiplication, to multiplication of matrices and to other problems which we will discuss below.\n\nAlgorithm\n\nRaising $a$ to the power of $n$ is expressed naively as multiplication by $a$ done $n - 1$ times: $a^{n} = a \\cdot a \\cdot \\ldots \\cdot a$. However, this approach is not practical for large $a$ or $n$.\n\n$a^{b+c} = a^b \\cdot a^c$ and $a^{2b} = a^b \\cdot a^b = (a^b)^2$.",
        "score": 1.0376791954040527,
        "is_relevant": false
      },
      {
        "text": "```cpp int gcd(int a, int b) { return a ? gcd(b % a, a) : b; }\n\nint powmod(int a, int b, int p) { int res = 1; while (b > 0) { if (b & 1) { res = res * a % p; } a = a * a % p; b >>= 1; } return res; }\n\n// Finds the primitive root modulo p int generator(int p) { vector\n\nfor (int res = 2; res <= p; ++res) {\n    bool ok = true;\n    for (int factor : fact) {\n        if (powmod(res, phi / factor, p) == 1) {\n            ok = false;\n            break;\n        }\n    }\n    if (ok) return res;\n}\nreturn -1;",
        "score": 1.040630578994751,
        "is_relevant": false
      }
    ]
  },
  {
    "query": "Efficient search in suffix trees",
    "chunk_scores": [
      {
        "text": "This takes $O(length(S))$ time for preprocessing and then $O(length(ans) \\cdot k)$ for each query (where $ans$ is the answer for the query and $k$ is the size of the alphabet).\n\nSmallest cyclic shift\n\nGiven a string $S$. We want to find the lexicographically smallest cyclic shift.\n\nWe construct a suffix automaton for the string $S + S$. Then the automaton will contain in itself as paths all the cyclic shifts of the string $S$.",
        "score": 0.8749905824661255,
        "is_relevant": true
      },
      {
        "text": "Overall, this requires $O(length (T))$ for preprocessing and $O(length(P) + answer(P))$ for each request, where $answer(P)$ — this is the size of the answer.\n\nFirst, we walk down the automaton for each character in the pattern to find our starting node requiring $O(length(P))$. Then, we use our workaround which will work in time $O(answer(P))$, because we will not visit a state twice (because only one suffix link leaves each state, so there cannot be two different paths leading to the same state).",
        "score": 0.93016517162323,
        "is_relevant": true
      },
      {
        "text": "tags: - Translated e_maxx_link: suffix_automata\n\nSuffix Automaton\n\nA suffix automaton is a powerful data structure that allows solving many string-related problems.\n\nFor example, you can search for all occurrences of one string in another, or count the amount of different substrings of a given string. Both tasks can be solved in linear time with the help of a suffix automaton.",
        "score": 0.9403403401374817,
        "is_relevant": true
      },
      {
        "text": "However this implementation is inefficient. It is easy to construct an example, so that the trees degenerate into long chains. In that case each call find_set(v) can take $O(n)$ time.\n\nThis is far away from the complexity that we want to have (nearly constant time). Therefore we will consider two optimizations that will allow to significantly accelerate the work.\n\nPath compression optimization\n\nThis optimization is designed for speeding up find_set.",
        "score": 0.9468538165092468,
        "is_relevant": false
      },
      {
        "text": "Suffix automaton for \"abcbc\" with suffix links\n\nRecap\n\nBefore proceeding to the algorithm itself, we recap the accumulated knowledge, and introduce a few auxiliary notations.\n\nThe substrings of the string $s$ can be decomposed into equivalence classes according to their end positions $endpos$.\n\nThe suffix automaton consists of the initial state $t_0$, as well as of one state for each $endpos$-equivalence class.",
        "score": 0.9547206163406372,
        "is_relevant": false
      },
      {
        "text": "We construct a suffix automaton for the string $S$.\n\nWe will now take the string $T$, and for each prefix look for the longest suffix of this prefix in $S$. In other words, for each position in the string $T$, we want to find the longest common substring of $S$ and $T$ ending in that position.\n\nFor this we will use two variables, the current state $v$, and the current length $l$. These two variables will describe the current matching part: its length and the state that corresponds to it.",
        "score": 0.9813700318336487,
        "is_relevant": true
      },
      {
        "text": "We construct a suffix automaton for the string $S + S$. Then the automaton will contain in itself as paths all the cyclic shifts of the string $S$.\n\nConsequently the problem is reduced to finding the lexicographically smallest path of length $length(S)$, which can be done in a trivial way: we start in the initial state and greedily pass through the transitions with the minimal character.\n\nTotal time complexity is $O(length(S))$.\n\nNumber of occurrences",
        "score": 0.9839158058166504,
        "is_relevant": true
      },
      {
        "text": "Intuitively a suffix automaton can be understood as a compressed form of all substrings of a given string. An impressive fact is, that the suffix automaton contains all this information in a highly compressed form. For a string of length $n$ it only requires $O(n)$ memory. Moreover, it can also be built in $O(n)$ time (if we consider the size $k$ of the alphabet as a constant), otherwise both the memory and the time complexity will be $O(n \\log k)$.",
        "score": 0.9871976375579834,
        "is_relevant": true
      },
      {
        "text": "Given a text $T$, and multiple patterns $P$. We have to check whether or not the strings $P$ appear as a substring of $T$.\n\nWe build a suffix automaton of the text $T$ in $O(length(T))$ time. To check if a pattern $P$ appears in $T$, we follow the transitions, starting from $t_0$, according to the characters of $P$. If at some point there doesn't exists a transition, then the pattern $P$ doesn't appear as a substring of $T$. If we can process the entire string $P$ this way, then the string appears in $T$.",
        "score": 0.9900310635566711,
        "is_relevant": true
      },
      {
        "text": "It is clear that this will take $O(length(P))$ time for each string $P$. Moreover the algorithm actually finds the length of the longest prefix of $P$ that appears in the text.\n\nNumber of different substrings\n\nGiven a string $S$. You want to compute the number of different substrings.\n\nLet us build a suffix automaton for the string $S$.",
        "score": 0.9915204644203186,
        "is_relevant": false
      }
    ]
  },
  {
    "query": "Range Minimum Query (RMQ) structure and usage",
    "chunk_scores": [
      {
        "text": "Let's denote with $A$ the array on which we want to perform the range minimum queries. And $N$ will be the size of $A$.",
        "score": 0.6759703159332275,
        "is_relevant": true
      },
      {
        "text": "Now we only have to learn how to quickly answer range minimum queries within each block. In fact if the received range minimum query is $[l, r]$ and $l$ and $r$ are in different blocks then the answer is the minimum of the following three values: the minimum of the suffix of block of $l$ starting at $l$, the minimum of the prefix of block of $r$ ending at $r$, and the minimum of the blocks between those. The minimum of the blocks in between can be answered in $O(1)$ using the Sparse Table. So this leaves",
        "score": 0.7361822128295898,
        "is_relevant": true
      },
      {
        "text": "The range minimum query [l, r] is equivalent to the lowest common ancestor query [l', r'], where l' is the node corresponding to the element A[l] and r' the node corresponding to the element A[r]. Indeed the node corresponding to the smallest element in the range has to be an ancestor of all nodes in the range, therefor also from l' and r'. This automatically follows from the min-heap property. And is also has to be the lowest ancestor, because otherwise l' and r' would be both in the left or in the right",
        "score": 0.7612184286117554,
        "is_relevant": true
      },
      {
        "text": "tags: - Translated e_maxx_link: rmq_linear\n\nSolve RMQ (Range Minimum Query) by finding LCA (Lowest Common Ancestor)\n\nGiven an array A[0..N-1]. For each query of the form [L, R] we want to find the minimum in the array A starting from position L and ending with position R. We will assume that the array A doesn't change in the process, i.e. this article describes a solution to the static RMQ problem",
        "score": 0.8718961477279663,
        "is_relevant": true
      },
      {
        "text": "You can read more about this reduction in the article Lowest Common Ancestor. In that article the minimum of a range was either found by sqrt-decomposition in $O(\\sqrt{N})$ or in $O(\\log N)$ using a Segment tree. In this article we look at how we can solve the given range minimum queries in $O(1)$ time, while still only taking $O(N)$ time for preprocessing.",
        "score": 0.8796120882034302,
        "is_relevant": true
      },
      {
        "text": "the minimum of the prefix of block of $r$ ending at $r$, and the minimum of the blocks between those. The minimum of the blocks in between can be answered in $O(1)$ using the Sparse Table. So this leaves us only the range minimum queries inside blocks.",
        "score": 0.8814963102340698,
        "is_relevant": true
      },
      {
        "text": "How can we answer a query RMQ in $O(1)$ using this data structure? Let the received query be $[l, r]$, then the answer is $\\min(T[l][\\text{sz}], T[r-2^{\\text{sz}}+1][\\text{sz}])$, where $\\text{sz}$ is the biggest exponent such that $2^{\\text{sz}}$ is not bigger than the range length $r-l+1$. Indeed we can take the range $[l, r]$ and cover it two segments of length $2^{\\text{sz}}$ - one starting in $l$ and the other ending in $r$. These segments overlap, but this doesn't interfere with our computation. To",
        "score": 0.9177412986755371,
        "is_relevant": true
      },
      {
        "text": "So we learned how to precompute range minimum queries within each block, as well as range minimum queries over a range of blocks, all in $O(N)$. With these precomputations we can answer each query in $O(1)$, by using at most four precomputed values: the minimum of the block containing l, the minimum of the block containing r, and the two minima of the overlapping segments of the blocks between them.\n\nImplementation\n\n```cpp int n; vector\n\nint block_size, block_cnt; vector",
        "score": 0.971990704536438,
        "is_relevant": true
      },
      {
        "text": "Thus the number of different blocks is $O(\\sqrt{N})$, and therefore we can precompute the results of range minimum queries inside all different blocks in $O(\\sqrt{N} K^2) = O(\\sqrt{N} \\log^2(N)) = O(N)$ time. For the implementation we can characterize a block by a bitmask of length $K-1$ (which will fit in a standard int) and store the index of the minimum in an array $\\text{block}[\\text{mask}][l][r]$ of size $O(\\sqrt{N} \\log^2(N))$.",
        "score": 1.066649079322815,
        "is_relevant": true
      },
      {
        "text": "There is an easy data structure that we can use for solving the RMQ problem with $O(N \\log N)$ preprocessing and $O(1)$ for each query: the Sparse Table. We create a table $T$ where each element $T[i][j]$ is equal to the minimum of $A$ in the interval $[i, i + 2^j - 1]$. Obviously $0 \\leq j \\leq \\lceil \\log N \\rceil$, and therefore the size of the Sparse Table will be $O(N \\log N)$. You can build the table easily in $O(N \\log N)$ by noting that $T[i][j] = \\min(T[i][j-1], T[i+2^{j-1}][j-1])$.",
        "score": 1.1018271446228027,
        "is_relevant": true
      }
    ]
  },
  {
    "query": "Minimum cost flow optimization",
    "chunk_scores": [
      {
        "text": "Implementation\n\n```cpp struct Edge { int a, b, cost; };\n\nint n; vector\n\nvoid solve() { vector\n\nfor (int i = 0; i < n; ++i) {\n    x = -1;\n    for (Edge e : edges) {\n        if (d[e.a] + e.cost < d[e.b]) {\n            d[e.b] = max(-INF, d[e.a] + e.cost);\n            p[e.b] = e.a;\n            x = e.b;\n        }\n    }\n}\n\nif (x == -1) {\n    cout << \"No negative cycle found.\";\n} else {\n    for (int i = 0; i < n; ++i)\n        x = p[x];",
        "score": 1.095931887626648,
        "is_relevant": true
      },
      {
        "text": "Now we give a full version of the solution, which additionally also finds the boundaries of the desired segment:\n\n```cpp int ans = a[0], ans_l = 0, ans_r = 0; int sum = 0, min_sum = 0, min_pos = -1;\n\nfor (int r = 0; r < n; ++r) { sum += a[r]; int cur = sum - min_sum; if (cur > ans) { ans = cur; ans_l = min_pos + 1; ans_r = r; } if (sum < min_sum) { min_sum = sum; min_pos = r; } } ```\n\nAlgorithm 2",
        "score": 1.1033380031585693,
        "is_relevant": false
      },
      {
        "text": "To minimize the runtime, we apply the idea behind divide and conquer. First, compute $opt(i, n / 2)$. Then, compute $opt(i, n / 4)$, knowing that it is less than or equal to $opt(i, n / 2)$ and $opt(i, 3 n / 4)$ knowing that it is greater than or equal to $opt(i, n / 2)$. By recursively keeping track of the lower and upper bounds on $opt$, we reach a $O(m n \\log n)$ runtime. Each possible value of $opt(i, j)$ only appears in $\\log n$ different nodes.",
        "score": 1.1424676179885864,
        "is_relevant": true
      },
      {
        "text": "Let $opt(i, j)$ be the value of $k$ that minimizes the above expression. Assuming that the cost function satisfies the quadrangle inequality, we can show that $opt(i, j) \\leq opt(i, j + 1)$ for all $i, j$. This is known as the monotonicity condition. Then, we can apply divide and conquer DP. The optimal \"splitting point\" for a fixed $i$ increases as $j$ increases.",
        "score": 1.1597249507904053,
        "is_relevant": true
      },
      {
        "text": "Do $N$ iterations of Bellman-Ford algorithm. If there were no changes on the last iteration, there is no cycle of negative weight in the graph. Otherwise take a vertex the distance to which has changed, and go from it via its ancestors until a cycle is found. This cycle will be the desired cycle of negative weight.\n\nImplementation\n\n```cpp struct Edge { int a, b, cost; };\n\nint n; vector\n\nvoid solve() { vector",
        "score": 1.1635345220565796,
        "is_relevant": false
      },
      {
        "text": "int n; vector\n\nconst int INF = 1e9;\n\nvoid shortest_paths(int v0, vector\n\nwhile (!q.empty()) {\n    int u = q.front();\n    q.pop_front();\n    m[u] = 0;\n    for (Edge e : adj[u]) {\n        if (d[e.to] > d[u] + e.w) {\n            d[e.to] = d[u] + e.w;\n            p[e.to] = u;\n            if (m[e.to] == 2) {\n                m[e.to] = 1;\n                q.push_back(e.to);\n            } else if (m[e.to] == 0) {\n                m[e.to] = 1;\n                q.push_front(e.to);\n            }\n        }\n    }\n}",
        "score": 1.166205644607544,
        "is_relevant": true
      },
      {
        "text": "We first give a solution that finds a simple numerical answer without finding the indices of the desired segment:\n\n```cpp int ans = a[0], sum = 0, min_sum = 0;\n\nfor (int r = 0; r < n; ++r) { sum += a[r]; ans = max(ans, sum - min_sum); min_sum = min(min_sum, sum); } ```\n\nNow we give a full version of the solution, which additionally also finds the boundaries of the desired segment:\n\n```cpp int ans = a[0], ans_l = 0, ans_r = 0; int sum = 0, min_sum = 0, min_pos = -1;",
        "score": 1.1906872987747192,
        "is_relevant": false
      },
      {
        "text": "return dp_before[n - 1];\n\n} ```\n\nThings to look out for\n\nThe greatest difficulty with Divide and Conquer DP problems is proving the monotonicity of $opt$. One special case where this is true is when the cost function satisfies the quadrangle inequality, i.e., $C(a, c) + C(b, d) \\leq C(a, d) + C(b, c)$ for all $a \\leq b \\leq c \\leq d$. Many Divide and Conquer DP problems can also be solved with the Convex Hull trick or vice-versa. It is useful to know and understand both!\n\nPractice Problems",
        "score": 1.1933746337890625,
        "is_relevant": false
      },
      {
        "text": "Here is a description of an asymptotically optimal solution. It stands apart from other solutions for the RMQ problem, since it is very different from them: it reduces the RMQ problem to the LCA problem, and then uses the Farach-Colton and Bender algorithm, which reduces the LCA problem back to a specialized RMQ problem and solves that.\n\nAlgorithm",
        "score": 1.1936087608337402,
        "is_relevant": false
      },
      {
        "text": "If $v$ belongs to $M_2$, then $v$ is inserted into the set $M_1$ by inserting it at the back of the queue. $d_v$ is set to $d_u + w$.\n\nIf $v$ belongs to $M_1$, then we try to improve the value of $d_v$: $d_v = \\min(d_v, d_u + w)$. Since $v$ is already in $M_1$, we don't need to insert it into $M_1$ and the queue.\n\nIf $v$ belongs to $M_0$, and if $d_v$ can be improved $d_v > d_u + w$, then we improve $d_v$ and insert the vertex $v$ back to the set $M_1$, placing it at the beginning of the queue.",
        "score": 1.194494605064392,
        "is_relevant": false
      }
    ]
  },
  {
    "query": "Ternary search and its applications",
    "chunk_scores": [
      {
        "text": "The simplest implementation",
        "score": 1.2621763944625854,
        "is_relevant": false
      },
      {
        "text": "We now construct a directed graph of these implications: for each variable $x$ there will be two vertices $v_x$ and $v_{\\lnot x}$. The edges will correspond to the implications.\n\nLet's look at the example in 2-CNF form:\n\n$$(a \\lor \\lnot b) \\land (\\lnot a \\lor b) \\land (\\lnot a \\lor \\lnot b) \\land (a \\lor \\lnot c)$$\n\nThe oriented graph will contain the following vertices and edges:",
        "score": 1.2887852191925049,
        "is_relevant": false
      },
      {
        "text": "One of the most powerful applications of DSU is that it allows you to store both as compressed and uncompressed trees. The compressed form can be used for merging of trees and for the verification if two vertices are in the same tree, and the uncompressed form can be used - for example - to search for paths between two given vertices, or other traversals of the tree structure.",
        "score": 1.3014456033706665,
        "is_relevant": false
      },
      {
        "text": "A good example of this application is the problem of painting subarrays. We have a segment of length $L$, each element initially has the color 0. We have to repaint the subarray $[l, r]$ with the color $c$ for each query $(l, r, c)$. At the end we want to find the final color of each cell. We assume that we know all the queries in advance, i.e. the task is offline.",
        "score": 1.3082563877105713,
        "is_relevant": false
      },
      {
        "text": "Here we describe long arithmetic for only non-negative integers. To extend the algorithms to handle negative integers one has to introduce and maintain additional \"negative number\" flag or use two's complement integer representation.\n\nData Structure\n\nWe'll store numbers as a vector<int>, in which each element is a single \"digit\" of the number.\n\ncpp typedef vector<int> lnum;\n\nTo improve performance we'll use $10^9$ as the base, so that each \"digit\" of the long number contains 9 decimal digits at once.",
        "score": 1.325057864189148,
        "is_relevant": false
      },
      {
        "text": "cout << \"Cycle found: \";\n    for (int v : cycle)\n        cout << v << \" \";\n    cout << endl;\n}\n\n} ```\n\nPractice problems:\n\nAtCoder : Reachability in Functional Graph\n\nCSES : Round Trip\n\nCSES : Round Trip II",
        "score": 1.3302557468414307,
        "is_relevant": false
      },
      {
        "text": "Unary operators\n\nNow suppose that the expression also contains unary operators (operators that take one argument). The unary plus and unary minus are common examples of such operators.\n\nOne of the differences in this case, is that we need to determine whether the current operator is a unary or a binary one.",
        "score": 1.3339159488677979,
        "is_relevant": false
      },
      {
        "text": "This code first marks all numbers except zero and one as potential prime numbers, then it begins the process of sifting composite numbers. For this it iterates over all numbers from $2$ to $n$. If the current number $i$ is a prime number, it marks all numbers that are multiples of $i$ as composite numbers, starting from $i^2$. This is already an optimization over naive way of implementing it, and is allowed as all smaller numbers that are multiples of $i$ necessary also have a prime factor which is less",
        "score": 1.336120843887329,
        "is_relevant": false
      },
      {
        "text": "This takes $O(length(S))$ time for preprocessing and then $O(length(ans) \\cdot k)$ for each query (where $ans$ is the answer for the query and $k$ is the size of the alphabet).\n\nSmallest cyclic shift\n\nGiven a string $S$. We want to find the lexicographically smallest cyclic shift.\n\nWe construct a suffix automaton for the string $S + S$. Then the automaton will contain in itself as paths all the cyclic shifts of the string $S$.",
        "score": 1.3455605506896973,
        "is_relevant": false
      },
      {
        "text": "One of the differences in this case, is that we need to determine whether the current operator is a unary or a binary one.\n\nYou can notice, that before an unary operator, there always is another operator or an opening parenthesis, or nothing at all (if it is at the very beginning of the expression). On the contrary before a binary operator there will always be an operand (number) or a closing parenthesis. Thus it is easy to flag whether the next operator can be unary or not.",
        "score": 1.349058747291565,
        "is_relevant": false
      }
    ]
  },
  {
    "query": "Cycle detection using Floyd's algorithm",
    "chunk_scores": [
      {
        "text": "if (x == -1) {\n    cout << \"No negative cycle found.\";\n} else {\n    for (int i = 0; i < n; ++i)\n        x = p[x];\n\n    vector<int> cycle;\n    for (int v = x;; v = p[v]) {\n        cycle.push_back(v);\n        if (v == x && cycle.size() > 1)\n            break;\n    }\n    reverse(cycle.begin(), cycle.end());\n\n    cout << \"Negative cycle: \";\n    for (int v : cycle)\n        cout << v << ' ';\n    cout << endl;\n}\n\n} ```\n\nUsing Floyd-Warshall algorithm",
        "score": 0.6227778196334839,
        "is_relevant": true
      },
      {
        "text": "Brent's algorithm\n\nBrent implements a similar method to Floyd, using two pointers. The difference being that instead of advancing the pointers by one and two places respectively, they are advanced by powers of two. As soon as $2^i$ is greater than $\\lambda$ and $\\mu$, we will find the cycle.\n\ntext function floyd(f, x0): tortoise = x0 hare = f(x0) l = 1 while tortoise != hare: tortoise = hare repeat l times: hare = f(hare) if tortoise == hare: return true l *= 2 return true",
        "score": 0.7861104607582092,
        "is_relevant": true
      },
      {
        "text": "cout << \"Negative cycle: \";\n    for (int v : cycle)\n        cout << v << ' ';\n    cout << endl;\n}\n\n} ```\n\nUsing Floyd-Warshall algorithm\n\nThe Floyd-Warshall algorithm allows to solve the second variation of the problem - finding all pairs of vertices $(i, j)$ which don't have a shortest path between them (i.e. a path of arbitrarily small weight exists).\n\nAgain, the details can be found in the Floyd-Warshall article, and here we describe only its application.",
        "score": 0.7896482348442078,
        "is_relevant": false
      },
      {
        "text": "It is convenient to use different algorithms to solve these two variations of the problem, so we'll discuss both of them here.\n\nUsing Bellman-Ford algorithm\n\nBellman-Ford algorithm allows you to check whether there exists a cycle of negative weight in the graph, and if it does, find one of these cycles.\n\nThe details of the algorithm are described in the article on the Bellman-Ford algorithm. Here we'll describe only its application to this problem.",
        "score": 0.8743559122085571,
        "is_relevant": false
      },
      {
        "text": "text function floyd(f, x0): tortoise = x0 hare = f(x0) while tortoise != hare: tortoise = f(tortoise) hare = f(f(hare)) return true\n\nImplementation\n\nFirst, here is an implementation using the Floyd's cycle-finding algorithm. The algorithm generally runs in $O(\\sqrt[4]{n} \\log(n))$ time.\n\n```{.cpp file=pollard_rho} long long mult(long long a, long long b, long long mod) { return (__int128)a * b % mod; }\n\nlong long f(long long x, long long c, long long mod) { return (mult(x, x, mod) + c) % mod; }",
        "score": 0.8755500316619873,
        "is_relevant": true
      },
      {
        "text": "void find_cycle() { visited.assign(n, false); parent.assign(n, -1); cycle_start = -1;\n\nfor (int v = 0; v < n; v++) {\n    if (!visited[v] && dfs(v, parent[v]))\n        break;\n}\n\nif (cycle_start == -1) {\n    cout << \"Acyclic\" << endl;\n} else {\n    vector<int> cycle;\n    cycle.push_back(cycle_start);\n    for (int v = cycle_end; v != cycle_start; v = parent[v])\n        cycle.push_back(v);\n    cycle.push_back(cycle_start);",
        "score": 0.8798058032989502,
        "is_relevant": true
      },
      {
        "text": "We will look for the Euler cycle exactly as described above (non-recursive version), and at the same time at the end of this algorithm we will check whether the graph was connected or not (if the graph was not connected, then at the end of the algorithm some edges will remain in the graph, and in this case we need to print $-1$). Finally, the program takes into account that there can be isolated vertices in the graph.",
        "score": 0.879857063293457,
        "is_relevant": false
      },
      {
        "text": "Therefore, if we find two indices $s$ and $t$ with $g = \\gcd(x_s - x_t, n) > 1$, we have found a cycle and also a factor $g$ of $n$. It is possible that $g = n$. In this case we haven't found a proper factor, so we must repeat the algorithm with a different parameter (different starting value $x_0$, different constant $c$ in the polynomial function $f$).\n\nTo find the cycle, we can use any common cycle detection algorithm.\n\nFloyd's cycle-finding algorithm",
        "score": 0.9102832078933716,
        "is_relevant": true
      },
      {
        "text": "void find_cycle() { color.assign(n, 0); parent.assign(n, -1); cycle_start = -1;\n\nfor (int v = 0; v < n; v++) {\n    if (color[v] == 0 && dfs(v))\n        break;\n}\n\nif (cycle_start == -1) {\n    cout << \"Acyclic\" << endl;\n} else {\n    vector<int> cycle;\n    cycle.push_back(cycle_start);\n    for (int v = cycle_end; v != cycle_start; v = parent[v])\n        cycle.push_back(v);\n    cycle.push_back(cycle_start);\n    reverse(cycle.begin(), cycle.end());",
        "score": 0.9138426184654236,
        "is_relevant": true
      },
      {
        "text": "Looking for all cycles and combining them can be done with a simple recursive procedure:\n\nnohighlight procedure FindEulerPath(V) 1. iterate through all the edges outgoing from vertex V; remove this edge from the graph, and call FindEulerPath from the second end of this edge; 2. add vertex V to the answer.\n\nThe complexity of this algorithm is obviously linear with respect to the number of edges.\n\nBut we can write the same algorithm in the non-recursive version:",
        "score": 0.9223352670669556,
        "is_relevant": false
      }
    ]
  },
  {
    "query": "Lyndon factorization and its applications",
    "chunk_scores": [
      {
        "text": "tags: - Original\n\nInteger factorization\n\nIn this article we list several algorithms for the factorization of integers, each of which can be either fast or varying levels of slow depending on their input.",
        "score": 0.8809972405433655,
        "is_relevant": false
      },
      {
        "text": "Long Integer Arithmetic for Factorization Representation\n\nThe idea is to store the integer as its factorization, i.e. the powers of primes which divide it.\n\nThis approach is very easy to implement, and allows to do multiplication and division easily (asymptotically faster than the classical method), but not addition or subtraction. It is also very memory-efficient compared to the classical approach.",
        "score": 0.9056332111358643,
        "is_relevant": true
      },
      {
        "text": "} ```\n\nThe combination of a trial division for small prime numbers together with Brent's version of Pollard's rho algorithm makes a very powerful factorization algorithm.\n\nPractice Problems\n\nSPOJ - FACT0\n\nSPOJ - FACT1\n\nSPOJ - FACT2\n\nGCPC 15 - Divisions",
        "score": 0.9864553213119507,
        "is_relevant": true
      },
      {
        "text": "This method allows to save memory compared to the classical approach (though the savings are not as dramatic as in factorization representation). Besides, it allows to perform fast addition, subtraction and multiplication in time proportional to the number of prime numbers used as modulos (see Chinese remainder theorem article for implementation).",
        "score": 1.0533418655395508,
        "is_relevant": false
      },
      {
        "text": "```\n\nObserve that this is a probabilistic algorithm. A consequence of this is that there is a possibility of the algorithm being unable to find a factor at all.\n\nThe complexity is $O(B \\log B \\log^2 n)$ per iteration.\n\nPollard's rho algorithm\n\nPollard's Rho Algorithm is yet another factorization algorithm from John Pollard.",
        "score": 1.0608031749725342,
        "is_relevant": false
      },
      {
        "text": "// compute a^M\n    for (int p : primes) {\n        if (p >= B)\n            continue;\n        long long p_power = 1;\n        while (p_power * p <= B)\n            p_power *= p;\n        a = power(a, p_power, n);\n\n        g = gcd(a - 1, n);\n        if (g > 1 && g < n)\n            return g;\n    }\n    B *= 2;\n}\nreturn 1;\n\n```\n\nObserve that this is a probabilistic algorithm. A consequence of this is that there is a possibility of the algorithm being unable to find a factor at all.",
        "score": 1.0807822942733765,
        "is_relevant": false
      },
      {
        "text": "cpp int fermat(int n) { int a = ceil(sqrt(n)); int b2 = a*a - n; int b = round(sqrt(b2)); while (b * b != b2) { a = a + 1; b2 = a*a - n; b = round(sqrt(b2)); } return a - b; }\n\nThis factorization method can be very fast if the difference between the two factors $p$ and $q$ is small. The algorithm runs in $O(|p - q|)$ time. In practice though, this method is rarely used. Once factors become further apart, it is extremely slow.",
        "score": 1.0837674140930176,
        "is_relevant": false
      },
      {
        "text": "$m$ be any positive integer. Suppose the prime factorization of $m$ is $m = \\prod {q_i}^{e_i}$, where each $q_i$ is a prime and $e_i \\geqslant 1$. Then $m$ is $\\mathrm{B}$-powersmooth if, for all $i$, ${q_i}^{e_i} \\leqslant \\mathrm{B}$. E.g. the prime factorization of $4817191$ is $1303 \\cdot 3697$. And the values, $1303 - 1$ and $3697 - 1$, are $31$-powersmooth and $16$-powersmooth respectively, because $1303 - 1 = 2 \\cdot 3 \\cdot 7 \\cdot 31$ and $3697 - 1 = 2^4 \\cdot 3 \\cdot 7 \\cdot 11$. In 1974 John",
        "score": 1.0952744483947754,
        "is_relevant": false
      },
      {
        "text": "Division by short integer\n\nDivide long integer $a$ by short integer $b$ ($b < base$), store integer result in $a$ and remainder in carry:\n\ncpp int carry = 0; for (int i=(int)a.size()-1; i>=0; --i) { long long cur = a[i] + carry * 1ll * base; a[i] = int (cur / b); carry = int (cur % b); } while (a.size() > 1 && a.back() == 0) a.pop_back();\n\nLong Integer Arithmetic for Factorization Representation\n\nThe idea is to store the integer as its factorization, i.e. the powers of primes which divide it.",
        "score": 1.1066104173660278,
        "is_relevant": false
      },
      {
        "text": "Some composite numbers don't have factors $p$ s.t. $p-1$ is $\\mathrm{B}$-powersmooth for small $\\mathrm{B}$. For example, for the composite number $100~000~000~000~000~493 = 763~013 \\cdot 131~059~365~961$, values $p-1$ are $190~753$-powersmooth and $1~092~161~383$-powersmooth correspondingly. We will have to choose $B \\geq 190~753$ to factorize the number.\n\nIn the following implementation we start with $\\mathrm{B} = 10$ and increase $\\mathrm{B}$ after each each iteration.",
        "score": 1.1263656616210938,
        "is_relevant": false
      }
    ]
  },
  {
    "query": "Manhattan distance calculation",
    "chunk_scores": [
      {
        "text": "Now to the algorithm. At each step three sets of vertices are maintained:\n\n$M_0$ - vertices, for which the distance has already been calculated (although it might not be the final distance)\n\n$M_1$ - vertices, for which the distance currently is calculated\n\n$M_2$ - vertices, for which the distance has not yet been calculated\n\nThe vertices in the set $M_1$ are stored in a bidirectional queue (deque).",
        "score": 1.1641244888305664,
        "is_relevant": false
      },
      {
        "text": "$M_2$ - vertices, for which the distance has not yet been calculated\n\nThe vertices in the set $M_1$ are stored in a bidirectional queue (deque).\n\nAt each step of the algorithm we take a vertex from the set $M_1$ (from the front of the queue). Let $u$ be the selected vertex. We put this vertex $u$ into the set $M_0$. Then we iterate over all edges coming out of this vertex. Let $v$ be the second end of the current edge, and $w$ its weight.",
        "score": 1.2637808322906494,
        "is_relevant": true
      },
      {
        "text": "long long f(long long x, long long c, long long mod) { return (mult(x, x, mod) + c) % mod; }\n\nlong long rho(long long n, long long x0=2, long long c=1) { long long x = x0; long long y = x0; long long g = 1; while (g == 1) { x = f(x, c, n); y = f(y, c, n); y = f(y, c, n); g = gcd(abs(x - y), n); } return g; } ```\n\nThe following table shows the values of $x$ and $y$ during the algorithm for $n = 2206637$, $x_0 = 2$ and $c = 1$.",
        "score": 1.3218834400177002,
        "is_relevant": false
      },
      {
        "text": "Description\n\nLet the array $d$ contain the shortest path lengths, i.e. $d_i$ is the current length of the shortest path from the vertex $v_0$ to the vertex $i$. Initially this array is filled with infinity for every vertex, except $d_{v_0} = 0$. After the algorithm finishes, this array will contain the shortest distances.",
        "score": 1.3238298892974854,
        "is_relevant": false
      },
      {
        "text": "int m = 128;\nint l = 1;\nwhile (g == 1) {\n    y = x;\n    for (int i = 1; i < l; i++)\n        x = f(x, c, n);\n    int k = 0;\n    while (k < l && g == 1) {\n        xs = x;\n        for (int i = 0; i < m && i < l - k; i++) {\n            x = f(x, c, n);\n            q = mult(q, abs(y - x), n);\n        }\n        g = gcd(q, n);\n        k += m;\n    }\n    l *= 2;\n}\nif (g == n) {\n    do {\n        xs = f(xs, c, n);\n        g = gcd(abs(xs - y), n);\n    } while (g == 1);\n}\nreturn g;\n\n} ```",
        "score": 1.3470001220703125,
        "is_relevant": false
      },
      {
        "text": "Let the array $p$ contain the current ancestors, i.e. $p_i$ is the direct ancestor of the vertex $i$ on the current shortest path from $v_0$ to $i$. Just like the array $d$, the array $p$ changes gradually during the algorithm and at the end takes its final values.\n\nNow to the algorithm. At each step three sets of vertices are maintained:\n\n$M_0$ - vertices, for which the distance has already been calculated (although it might not be the final distance)",
        "score": 1.3553845882415771,
        "is_relevant": false
      },
      {
        "text": "static u256 mult(u128 x, u128 y) {\n    u64 a = x >> 64, b = x;\n    u64 c = y >> 64, d = y;\n    // (a*2^64 + b) * (c*2^64 + d) =\n    // (a*c) * 2^128 + (a*d + b*c)*2^64 + (b*d)\n    u128 ac = (u128)a * c;\n    u128 ad = (u128)a * d;\n    u128 bc = (u128)b * c;\n    u128 bd = (u128)b * d;\n    u128 carry = (u128)(u64)ad + (u128)(u64)bc + (bd >> 64u);\n    u128 high = ac + (ad >> 64u) + (bc >> 64u) + (carry >> 64u);\n    u128 low = (ad << 64u) + (bc << 64u) + bd;\n    return {high, low};\n}\n\n};",
        "score": 1.3815369606018066,
        "is_relevant": false
      },
      {
        "text": "Support distances up to representative\n\nSometimes in specific applications of the DSU you need to maintain the distance between a vertex and the representative of its set (i.e. the path length in the tree from the current node to the root of the tree).\n\nIf we don't use path compression, the distance is just the number of recursive calls. But this will be inefficient.\n\nHowever it is possible to do path compression, if we store the distance to the parent as additional information for each node.",
        "score": 1.3816393613815308,
        "is_relevant": false
      },
      {
        "text": "Fast application of a set of geometric operations to a set of points",
        "score": 1.4260393381118774,
        "is_relevant": true
      },
      {
        "text": "We already described how to compute $d[v]$ in the previous task. The value $ans[v]$ can be computed using the recursion:\n\n$$ans[v] = \\sum_{w : (v, w, c) \\in DAWG} d[w] + ans[w]$$\n\nWe take the answer of each adjacent vertex $w$, and add to it $d[w]$ (since every substring is one character longer when starting from the state $v$).\n\nAgain this task can be computed in $O(length(S))$ time.",
        "score": 1.4449365139007568,
        "is_relevant": false
      }
    ]
  },
  {
    "query": "Comparison of Bellman-Ford and Dijkstra algorithms",
    "chunk_scores": [
      {
        "text": "} ```\n\nComplexity\n\nThe algorithm usually performs quite fast - in most cases, even faster than Dijkstra's algorithm. However there exist cases for which the algorithm takes exponential time, making it unsuitable in the worst-case. See discussions on Stack Overflow and Codeforces for reference.",
        "score": 0.7047027349472046,
        "is_relevant": true
      },
      {
        "text": "The standard implementation of Bellman-Ford looks for a negative cycle reachable from some starting vertex $v$ ; however, the algorithm can be modified to just looking for any negative cycle in the graph. For this we need to put all the distance $d[i]$ to zero and not infinity — as if we are looking for the shortest path from all vertices simultaneously; the validity of the detection of a negative cycle is not affected.",
        "score": 0.8483790755271912,
        "is_relevant": true
      },
      {
        "text": "tags: - Translated e_maxx_link: levit_algorithm\n\nD´Esopo-Pape algorithm\n\nGiven a graph with $n$ vertices and $m$ edges with weights $w_i$ and a starting vertex $v_0$. The task is to find the shortest path from the vertex $v_0$ to every other vertex.\n\nThe algorithm from D´Esopo-Pape will work faster than Dijkstra's algorithm and the Bellman-Ford algorithm in most cases, and will also work for negative edges. However not for negative cycles.\n\nDescription",
        "score": 1.0502957105636597,
        "is_relevant": true
      },
      {
        "text": "Do $N$ iterations of Bellman-Ford algorithm. If there were no changes on the last iteration, there is no cycle of negative weight in the graph. Otherwise take a vertex the distance to which has changed, and go from it via its ancestors until a cycle is found. This cycle will be the desired cycle of negative weight.\n\nImplementation\n\n```cpp struct Edge { int a, b, cost; };\n\nint n; vector\n\nvoid solve() { vector",
        "score": 1.0695219039916992,
        "is_relevant": true
      },
      {
        "text": "It is easy to check the equivalence of these two forms of the algorithm. However, the second form is obviously faster, and the code will be much more efficient.\n\nThe Domino problem\n\nWe give here a classical Eulerian cycle problem - the Domino problem.",
        "score": 1.1277577877044678,
        "is_relevant": false
      },
      {
        "text": "Run Floyd-Warshall algorithm on the graph. Initially $d[v][v] = 0$ for each $v$. But after running the algorithm $d[v][v]$ will be smaller than $0$ if there exists a negative length path from $v$ to $v$. We can use this to also find all pairs of vertices that don't have a shortest path between them. We iterate over all pairs of vertices $(i, j)$ and for each pair we check whether they have a shortest path between them. To do this try all possibilities for an intermediate vertex $t$. $(i, j)$ doesn't have a",
        "score": 1.1412115097045898,
        "is_relevant": false
      },
      {
        "text": "It is convenient to use different algorithms to solve these two variations of the problem, so we'll discuss both of them here.\n\nUsing Bellman-Ford algorithm\n\nBellman-Ford algorithm allows you to check whether there exists a cycle of negative weight in the graph, and if it does, find one of these cycles.\n\nThe details of the algorithm are described in the article on the Bellman-Ford algorithm. Here we'll describe only its application to this problem.",
        "score": 1.1616883277893066,
        "is_relevant": true
      },
      {
        "text": "Now to the algorithm. At each step three sets of vertices are maintained:\n\n$M_0$ - vertices, for which the distance has already been calculated (although it might not be the final distance)\n\n$M_1$ - vertices, for which the distance currently is calculated\n\n$M_2$ - vertices, for which the distance has not yet been calculated\n\nThe vertices in the set $M_1$ are stored in a bidirectional queue (deque).",
        "score": 1.1657248735427856,
        "is_relevant": true
      },
      {
        "text": "over all pairs of vertices $(i, j)$ and for each pair we check whether they have a shortest path between them. To do this try all possibilities for an intermediate vertex $t$. $(i, j)$ doesn't have a shortest path, if one of the intermediate vertices $t$ has $d[t][t] < 0$ (i.e. $t$ is part of a cycle of negative weight), $t$ can be reached from $i$ and $j$ can be reached from $t$. Then the path from $i$ to $j$ can have arbitrarily small weight. We will denote this with -INF.",
        "score": 1.1720967292785645,
        "is_relevant": true
      },
      {
        "text": "return dp_before[n - 1];\n\n} ```\n\nThings to look out for\n\nThe greatest difficulty with Divide and Conquer DP problems is proving the monotonicity of $opt$. One special case where this is true is when the cost function satisfies the quadrangle inequality, i.e., $C(a, c) + C(b, d) \\leq C(a, d) + C(b, c)$ for all $a \\leq b \\leq c \\leq d$. Many Divide and Conquer DP problems can also be solved with the Convex Hull trick or vice-versa. It is useful to know and understand both!\n\nPractice Problems",
        "score": 1.1736860275268555,
        "is_relevant": false
      }
    ]
  },
  {
    "query": "Rank of a matrix and how to compute it",
    "chunk_scores": [
      {
        "text": "This matrix is called the Vandermonde matrix.\n\nThus we can compute the vector $(a_0, a_1, \\dots, a_{n-1})$ by multiplying the vector $(y_0, y_1, \\dots y_{n-1})$ from the left with the inverse of the matrix:",
        "score": 1.1986465454101562,
        "is_relevant": false
      },
      {
        "text": "Using the solution for the one-dimensional case, it is easy to obtain a solution in $O(n^3)$ for the two-dimensions case: we iterate over all possible values of $l_1$ and $r_1$, and calculate the sums from $l_1$ to $r_1$ in each row of the matrix. Now we have the one-dimensional problem of finding the indices $l_2$ and $r_2$ in this array, which can already be solved in linear time.",
        "score": 1.292715311050415,
        "is_relevant": false
      },
      {
        "text": "Two-dimensional case of the problem: search for maximum/minimum submatrix\n\nThe problem described in this article is naturally generalized to large dimensions. For example, in a two-dimensional case, it turns into a search for such a submatrix $[l_1 \\ldots r_1, l_2 \\ldots r_2]$ of a given matrix, which has the maximum sum of numbers in it.",
        "score": 1.3243646621704102,
        "is_relevant": false
      },
      {
        "text": "```cpp void make_set(int v) { parent[v] = make_pair(v, 0); rank[v] = 0; bipartite[v] = true; }\n\npair\n\nvoid add_edge(int a, int b) { pair\n\npair<int, int> pb = find_set(b);\nb = pb.first;\nint y = pb.second;\n\nif (a == b) {\n    if (x == y)\n        bipartite[a] = false;\n} else {\n    if (rank[a] < rank[b])\n        swap (a, b);\n    parent[b] = make_pair(a, x^y^1);\n    bipartite[a] &= bipartite[b];\n    if (rank[a] == rank[b])\n        ++rank[a];\n}",
        "score": 1.3386032581329346,
        "is_relevant": false
      },
      {
        "text": "This approach obviously only works offline, i.e. if we know all queries beforehand.\n\nIt is easy to see that we can apply path compression. And we can also use Union by rank, if we store the actual leader in an separate array.\n\n```cpp struct Query { int L, R, idx; };\n\nvector\n\ncontainer[i] contains all queries with R == i.",
        "score": 1.3571522235870361,
        "is_relevant": false
      },
      {
        "text": "the matrix that you get after removing the $i$-th row and $j$-th column. So you can, for example, delete the last row and last column of the matrix $L$, and the absolute value of the determinant of the resulting matrix will give you the number of spanning trees.",
        "score": 1.360231876373291,
        "is_relevant": false
      },
      {
        "text": "The following formula was proven by Kirchhoff in 1847.\n\nKirchhoff's matrix tree theorem\n\nLet $A$ be the adjacency matrix of the graph: $A_{u,v}$ is the number of edges between $u$ and $v$. Let $D$ be the degree matrix of the graph: a diagonal matrix with $D_{u,u}$ being the degree of vertex $u$ (including multiple edges and loops - edges which connect vertex $u$ with itself).",
        "score": 1.3792173862457275,
        "is_relevant": false
      },
      {
        "text": "the weight of the edge from $i$ to $j$, or $\\infty$ if there is no such edge. Instead of the usual operation of multiplying two matrices, a modified one should be used: instead of multiplication, both values are added, and instead of a summation, a minimum is taken. That is: $result_{ij} = \\min\\limits_{1\\ \\leq\\ k\\ \\leq\\ n}(a_{ik} + b_{kj})$.",
        "score": 1.3933780193328857,
        "is_relevant": false
      },
      {
        "text": "The problem can also be solved by DFS or BFS, but the method described here has an advantage: it can process the matrix row by row (i.e. to process a row we only need the previous and the current row, and only need a DSU built for the elements of one row) in $O(\\min(n, m))$ memory.\n\nStore additional information for each set\n\nDSU allows you to easily store additional information in the sets.",
        "score": 1.4069840908050537,
        "is_relevant": false
      },
      {
        "text": "```{.cpp file=divide_and_conquer_dp} int m, n; vector\n\nlong long C(int i, int j);\n\n// compute dp_cur[l], ... dp_cur[r] (inclusive) void compute(int l, int r, int optl, int optr) { if (l > r) return;\n\nint mid = (l + r) >> 1;\npair<long long, int> best = {LLONG_MAX, -1};\n\nfor (int k = optl; k <= min(mid, optr); k++) {\n    best = min(best, {(k ? dp_before[k - 1] : 0) + C(k, mid), k});\n}\n\ndp_cur[mid] = best.first;\nint opt = best.second;\n\ncompute(l, mid - 1, optl, opt);\ncompute(mid + 1, r, opt, optr);",
        "score": 1.4475407600402832,
        "is_relevant": false
      }
    ]
  },
  {
    "query": "Tree painting algorithms",
    "chunk_scores": [
      {
        "text": "This algorithm can be implemented easily in $O(n \\log n)$: we use a data structure that supports extracting the minimum (for example set<> or priority_queue<> in C++) to store all the leaf vertices.\n\nThe following implementation returns the list of edges corresponding to the tree.\n\n```{.cpp file=pruefer_decode_slow} vector\n\nset<int> leaves;\nfor (int i = 0; i < n; i++) {\n    if (degree[i] == 1)\n        leaves.insert(i);\n}",
        "score": 0.9096102118492126,
        "is_relevant": true
      },
      {
        "text": "There are many possible heuristics that can be used. Most popular are the following two approaches: In the first approach we use the size of the trees as rank, and in the second one we use the depth of the tree (more precisely, the upper bound on the tree depth, because the depth will get smaller when applying path compression).\n\nIn both approaches the essence of the optimization is the same: we attach the tree with the lower rank to the one with the bigger rank.",
        "score": 0.9519456624984741,
        "is_relevant": true
      },
      {
        "text": "Thus the Prüfer code for a given tree is a sequence of $n - 2$ numbers, where each number is the number of the connected vertex, i.e. this number is in the interval $[0, n-1]$.\n\nThe algorithm for computing the Prüfer code can be implemented easily with $O(n \\log n)$ time complexity, simply by using a data structure to extract the minimum (for instance set or priority_queue in C++), which contains a list of all the current leafs.\n\n```{.cpp file=pruefer_code_slow} vector\n\nvector",
        "score": 0.9848117232322693,
        "is_relevant": true
      },
      {
        "text": "Implementation\n\nHere is an implementation for directed graph.\n\n```cpp int n; vector\n\nbool dfs(int v) { color[v] = 1; for (int u : adj[v]) { if (color[u] == 0) { parent[u] = v; if (dfs(u)) return true; } else if (color[u] == 1) { cycle_end = v; cycle_start = u; return true; } } color[v] = 2; return false; }\n\nvoid find_cycle() { color.assign(n, 0); parent.assign(n, -1); cycle_start = -1;\n\nfor (int v = 0; v < n; v++) {\n    if (color[v] == 0 && dfs(v))\n        break;\n}",
        "score": 0.9961175918579102,
        "is_relevant": false
      },
      {
        "text": "```{.cpp file=pruefer_decode_fast} vector\n\nint ptr = 0;\nwhile (degree[ptr] != 1)\n    ptr++;\nint leaf = ptr;\n\nvector<pair<int, int>> edges;\nfor (int v : code) {\n    edges.emplace_back(leaf, v);\n    if (--degree[v] == 1 && v < ptr) {\n        leaf = v;\n    } else {\n        ptr++;\n        while (degree[ptr] != 1)\n            ptr++;\n        leaf = ptr;\n    }\n}\nedges.emplace_back(leaf, n-1);\nreturn edges;\n\n} ```\n\nBijection between trees and Prüfer codes",
        "score": 1.0052604675292969,
        "is_relevant": true
      },
      {
        "text": "In the end we only have two vertices left with degree equal to $1$. These are the vertices that didn't got removed by the Prüfer code process. We connect them to get the last edge of the tree. One of them will always be the vertex $n-1$.\n\nThis algorithm can be implemented easily in $O(n \\log n)$: we use a data structure that supports extracting the minimum (for example set<> or priority_queue<> in C++) to store all the leaf vertices.",
        "score": 1.0157470703125,
        "is_relevant": false
      },
      {
        "text": "Algorithm\n\nWe construct a Cartesian tree from the array A. A Cartesian tree of an array A is a binary tree with the min-heap property (the value of parent node has to be smaller or equal than the value of its children) such that the in-order traversal of the tree visits the nodes in the same order as they are in the array A.",
        "score": 1.0233842134475708,
        "is_relevant": false
      },
      {
        "text": "Now to solve this problem, we consider the queries in the reverse order: from last to first. This way when we execute a query, we only have to paint exactly the unpainted cells in the subarray $[l, r]$. All other cells already contain their final color. To quickly iterate over all unpainted cells, we use the DSU. We find the left-most unpainted cell inside of a segment, repaint it, and with the pointer we move to the next empty cell to the right.",
        "score": 1.0475794076919556,
        "is_relevant": false
      },
      {
        "text": "return code;\n\n} ```\n\nHowever the construction can also be implemented in linear time. Such an approach is described in the next section.\n\nBuilding the Prüfer code for a given tree in linear time\n\nThe essence of the algorithm is to use a moving pointer, which will always point to the current leaf vertex that we want to remove.",
        "score": 1.0549252033233643,
        "is_relevant": false
      },
      {
        "text": "Here we can directly apply the data structure, and get a solution that handles an addition of a vertex or an edge and a query in nearly constant time on average.\n\nThis application is quite important, because nearly the same problem appears in Kruskal's algorithm for finding a minimum spanning tree. Using DSU we can improve the $O(m \\log n + n^2)$ complexity to $O(m \\log n)$.\n\nSearch for connected components in an image",
        "score": 1.0552089214324951,
        "is_relevant": true
      }
    ]
  },
  {
    "query": "Bipartite graph checking",
    "chunk_scores": [
      {
        "text": "To solve this problem, we make a DSU for storing of the components and store the parity of the path up to the representative for each vertex. Thus we can quickly check if adding an edge leads to a violation of the bipartiteness or not: namely if the ends of the edge lie in the same connected component and have the same parity length to the leader, then adding this edge will produce a cycle of odd length, and the component will lose the bipartiteness property.",
        "score": 0.7959136962890625,
        "is_relevant": true
      },
      {
        "text": "```cpp void make_set(int v) { parent[v] = make_pair(v, 0); rank[v] = 0; }\n\npair\n\nvoid union_sets(int a, int b) { a = find_set(a).first; b = find_set(b).first; if (a != b) { if (rank[a] < rank[b]) swap(a, b); parent[b] = make_pair(a, 1); if (rank[a] == rank[b]) rank[a]++; } } ```\n\nSupport the parity of the path length / Checking bipartiteness online",
        "score": 0.9177283644676208,
        "is_relevant": true
      },
      {
        "text": "```cpp void make_set(int v) { parent[v] = make_pair(v, 0); rank[v] = 0; bipartite[v] = true; }\n\npair\n\nvoid add_edge(int a, int b) { pair\n\npair<int, int> pb = find_set(b);\nb = pb.first;\nint y = pb.second;\n\nif (a == b) {\n    if (x == y)\n        bipartite[a] = false;\n} else {\n    if (rank[a] < rank[b])\n        swap (a, b);\n    parent[b] = make_pair(a, x^y^1);\n    bipartite[a] &= bipartite[b];\n    if (rank[a] == rank[b])\n        ++rank[a];\n}",
        "score": 0.9219164848327637,
        "is_relevant": true
      },
      {
        "text": "$$t = x \\oplus y \\oplus 1$$\n\nThus regardless of how many joins we perform, the parity of the edges is carried from one leader to another.\n\nWe give the implementation of the DSU that supports parity. As in the previous section we use a pair to store the ancestor and the parity. In addition for each set we store in the array bipartite[] whether it is still bipartite or not.\n\n```cpp void make_set(int v) { parent[v] = make_pair(v, 0); rank[v] = 0; bipartite[v] = true; }\n\npair",
        "score": 0.9635351300239563,
        "is_relevant": true
      },
      {
        "text": "bool is_bipartite(int v) { return bipartite[find_set(v).first]; } ```\n\nOffline RMQ (range minimum query) in $O(\\alpha(n))$ on average / Arpa's trick { #arpa data-toc-label=\"Offline RMQ / Arpa's trick\"}\n\nWe are given an array a[] and we have to compute some minima in given segments of the array.",
        "score": 1.0445420742034912,
        "is_relevant": true
      },
      {
        "text": "This criterion can be verified in $O(n + m)$ time by finding all strongly connected components.\n\nThe following image shows all strongly connected components for the example. As we can check easily, neither of the four components contain a vertex $x$ and its negation $\\lnot x$, therefore the example has a solution. We will learn in the next paragraphs how to compute a valid assignment, but just for demonstration purposes the solution $a = \\text{false}$, $b = \\text{false}$, $c = \\text{false}$ is given.",
        "score": 1.0735316276550293,
        "is_relevant": false
      },
      {
        "text": "So we have constructed an algorithm that finds the required values of variables under the assumption that for any variable $x$ the vertices $x$ and $\\lnot x$ are in different strongly connected components. Above showed the correctness of this algorithm. Consequently we simultaneously proved the above criterion for the existence of a solution.\n\nImplementation:",
        "score": 1.101930856704712,
        "is_relevant": false
      },
      {
        "text": "We will look for the Euler cycle exactly as described above (non-recursive version), and at the same time at the end of this algorithm we will check whether the graph was connected or not (if the graph was not connected, then at the end of the algorithm some edges will remain in the graph, and in this case we need to print $-1$). Finally, the program takes into account that there can be isolated vertices in the graph.",
        "score": 1.1046643257141113,
        "is_relevant": false
      },
      {
        "text": "Run Floyd-Warshall algorithm on the graph. Initially $d[v][v] = 0$ for each $v$. But after running the algorithm $d[v][v]$ will be smaller than $0$ if there exists a negative length path from $v$ to $v$. We can use this to also find all pairs of vertices that don't have a shortest path between them. We iterate over all pairs of vertices $(i, j)$ and for each pair we check whether they have a shortest path between them. To do this try all possibilities for an intermediate vertex $t$. $(i, j)$ doesn't have a",
        "score": 1.1258554458618164,
        "is_relevant": false
      },
      {
        "text": "Support the parity of the path length / Checking bipartiteness online\n\nIn the same way as computing the path length to the leader, it is possible to maintain the parity of the length of the path before him. Why is this application in a separate paragraph?",
        "score": 1.13690185546875,
        "is_relevant": true
      }
    ]
  },
  {
    "query": "Montgomery modular arithmetic",
    "chunk_scores": [
      {
        "text": "In the following code we initialize r2 with -n % n, which is equivalent to $r - n \\equiv r \\bmod n$, shift it 4 times to get $r \\cdot 2^4 \\bmod n$. This number can be interpreted as $2^4$ in Montgomery space. If we square it $5$ times, we get $(2^4)^{2^5} = (2^4)^{32} = 2^{128} = r$ in Montgomery space, which is exactly $r^2 \\bmod n$.\n\n``` struct Montgomery { Montgomery(u128 n) : mod(n), inv(1), r2(-n % n) { for (int i = 0; i < 7; i++) inv *= 2 - n * inv;",
        "score": 0.6151573657989502,
        "is_relevant": true
      },
      {
        "text": "``` struct Montgomery { Montgomery(u128 n) : mod(n), inv(1), r2(-n % n) { for (int i = 0; i < 7; i++) inv *= 2 - n * inv;\n\n    for (int i = 0; i < 4; i++) {\n        r2 <<= 1;\n        if (r2 >= mod)\n            r2 -= mod;\n    }\n    for (int i = 0; i < 5; i++)\n        r2 = mul(r2, r2);\n}\n\nu128 init(u128 x) {\n    return mult(x, r2);\n}\n\nu128 mod, inv, r2;\n\n}; ```",
        "score": 0.6516839265823364,
        "is_relevant": true
      },
      {
        "text": "As we see, we can perform the Montgomery reduction without any heavy modulo operations. If we choose $r$ as a power of $2$, the modulo operations and divisions in the algorithm can be computed using bitmasking and shifting.\n\nA second application of the Montgomery reduction is to transfer a number back from the Montgomery space into the normal space.\n\nFast inverse trick",
        "score": 0.6596359014511108,
        "is_relevant": true
      },
      {
        "text": "};\n\nstruct Montgomery { Montgomery(u128 n) : mod(n), inv(1) { for (int i = 0; i < 7; i++) inv *= 2 - n * inv; }\n\nu128 init(u128 x) {\n    x %= mod;\n    for (int i = 0; i < 128; i++) {\n        x <<= 1;\n        if (x >= mod)\n            x -= mod;\n    }\n    return x;\n}\n\nu128 reduce(u256 x) {\n    u128 q = x.low * inv;\n    i128 a = x.high - u256::mult(q, mod).high;\n    if (a < 0)\n        a += mod;\n    return a;\n}\n\nu128 mult(u128 a, u128 b) {\n    return reduce(u256::mult(a, b));\n}\n\nu128 mod, inv;\n\n}; ```",
        "score": 0.6624659895896912,
        "is_relevant": true
      },
      {
        "text": "Therefore the multiplication in the Montgomery space is defined as:\n\n$$\\bar{x} * \\bar{y} := \\bar{x} \\cdot \\bar{y} \\cdot r^{-1} \\bmod n.$$\n\nMontgomery reduction\n\nThe multiplication of two numbers in the Montgomery space requires an efficient computation of $x \\cdot r^{-1} \\bmod n$. This operation is called the Montgomery reduction, and is also known as the algorithm REDC.\n\nBecause $\\gcd(n, r) = 1$, we know that there are two numbers $r^{-1}$ and $n^{\\prime}$ with $0 < r^{-1}, n^{\\prime} < n$ with",
        "score": 0.6833162307739258,
        "is_relevant": true
      },
      {
        "text": "The Montgomery (modular) multiplication is a method that allows computing such multiplications faster. Instead of dividing the product and subtracting $n$ multiple times, it adds multiples of $n$ to cancel out the lower bits and then just discards the lower bits.\n\nMontgomery representation\n\nHowever the Montgomery multiplication doesn't come for free. The algorithm works only in the Montgomery space. And we need to transform our numbers into that space, before we can start multiplying.",
        "score": 0.6922602653503418,
        "is_relevant": true
      },
      {
        "text": "However this is not the case for multiplication.\n\nWe expect the result to be:\n\n$$\\bar{x} * \\bar{y} = \\overline{x \\cdot y} = (x \\cdot y) \\cdot r \\bmod n.$$\n\nBut the normal multiplication will give us:\n\n$$\\bar{x} \\cdot \\bar{y} = (x \\cdot y) \\cdot r \\cdot r \\bmod n.$$\n\nTherefore the multiplication in the Montgomery space is defined as:\n\n$$\\bar{x} * \\bar{y} := \\bar{x} \\cdot \\bar{y} \\cdot r^{-1} \\bmod n.$$\n\nMontgomery reduction",
        "score": 0.7016357779502869,
        "is_relevant": true
      },
      {
        "text": "u128 mult(u128 a, u128 b) {\n    return reduce(u256::mult(a, b));\n}\n\nu128 mod, inv;\n\n}; ```\n\nFast transformation\n\nThe current method of transforming a number into Montgomery space is pretty slow. There are faster ways.\n\nYou can notice the following relation:\n\n$$\\bar{x} := x \\cdot r \\bmod n = x \\cdot r^2 / r = x * r^2$$",
        "score": 0.710381031036377,
        "is_relevant": false
      },
      {
        "text": "tags: - Original\n\nMontgomery Multiplication\n\nMany algorithms in number theory, like prime testing or integer factorization, and in cryptography, like RSA, require lots of operations modulo a large number. A multiplications like $x y \\bmod{n}$ is quite slow to compute with the typical algorithms, since it requires a division to know how many times $n$ has to be subtracted from the product. And division is a really expensive operation, especially with big numbers.",
        "score": 0.7214078903198242,
        "is_relevant": true
      },
      {
        "text": "Inside the Montgomery space you can still perform most operations as usual. You can add two elements ($x \\cdot r + y \\cdot r \\equiv (x + y) \\cdot r \\bmod n$), subtract, check for equality, and even compute the greatest common divisor of a number with $n$ (since $\\gcd(n, r) = 1$). All with the usual algorithms.\n\nHowever this is not the case for multiplication.\n\nWe expect the result to be:\n\n$$\\bar{x} * \\bar{y} = \\overline{x \\cdot y} = (x \\cdot y) \\cdot r \\bmod n.$$",
        "score": 0.7265908122062683,
        "is_relevant": true
      }
    ]
  },
  {
    "query": "Fast Fourier Transform and polynomial multiplication",
    "chunk_scores": [
      {
        "text": "Fast Fourier Transform\n\nThe fast Fourier transform is a method that allows computing the DFT in $O(n \\log n)$ time. The basic idea of the FFT is to apply divide and conquer. We divide the coefficient vector of the polynomial into two vectors, recursively compute the DFT for each of them, and combine the results to compute the DFT of the complete polynomial.\n\nSo let there be a polynomial $A(x)$ with degree $n - 1$, where $n$ is a power of $2$, and $n > 1$:",
        "score": 0.5971700549125671,
        "is_relevant": true
      },
      {
        "text": "tags: - Translated e_maxx_link: fft_multiply\n\nFast Fourier transform\n\nIn this article we will discuss an algorithm that allows us to multiply two polynomials of length $n$ in $O(n \\log n)$ time, which is better than the trivial multiplication which takes $O(n^2)$ time. Obviously also multiplying two long numbers can be reduced to multiplying polynomials, so also two long numbers can be multiplied in $O(n \\log n)$ time (where $n$ is the number of digits in the numbers).",
        "score": 0.6511083841323853,
        "is_relevant": true
      },
      {
        "text": "This approach therefore requires computing the products of polynomials with smaller coefficients (by using the normal FFT and inverse FFT), and then the original product can be restored using modular addition and multiplication in $O(n)$ time.\n\nApplications\n\nDFT can be used in a huge variety of other problems, which at the first glance have nothing to do with multiplying polynomials.\n\nAll possible sums",
        "score": 0.6749029159545898,
        "is_relevant": true
      },
      {
        "text": "It should be noted, that the two polynomials should have the same degree. Otherwise the two result vectors of the DFT have different length. We can accomplish this by adding coefficients with the value $0$.\n\nAnd also, since the result of the product of two polynomials is a polynomial of degree $2 (n - 1)$, we have to double the degrees of each polynomial (again by padding $0$s). From a vector with $n$ values we cannot reconstruct the desired polynomial with $2n - 1$ coefficients.\n\nFast Fourier Transform",
        "score": 0.7207371592521667,
        "is_relevant": true
      },
      {
        "text": "Application of the DFT: fast multiplication of polynomials\n\nLet there be two polynomials $A$ and $B$. We compute the DFT for each of them: $\\text{DFT}(A)$ and $\\text{DFT}(B)$.\n\nWhat happens if we multiply these polynomials? Obviously at each point the values are simply multiplied, i.e.\n\n$$(A \\cdot B)(x) = A(x) \\cdot B(x).$$",
        "score": 0.7321745157241821,
        "is_relevant": true
      },
      {
        "text": "Notice, that the FFT algorithm presented here runs in $O(n \\log n)$ time, but it doesn't work for multiplying arbitrary big polynomials with arbitrary large coefficients or for multiplying arbitrary big integers. It can easily handle polynomials of size $10^5$ with small coefficients, or multiplying two numbers of size $10^6$, which is usually enough for solving competitive programming problems. Beyond the scale of multiplying numbers with $10^6$ bits, the range and precision of the floating point numbers",
        "score": 0.7953715324401855,
        "is_relevant": true
      },
      {
        "text": "Now we switch the objective a little bit. We still want to multiply two polynomials in $O(n \\log n)$ time, but this time we want to compute the coefficients modulo some prime number $p$. Of course for this task we can use the normal DFT and apply the modulo operator to the result. However, doing so might lead to rounding errors, especially when dealing with large numbers. The number theoretic transform (NTT) has the advantage, that it only works with integer, and therefore the result are guaranteed to be",
        "score": 0.8817108273506165,
        "is_relevant": true
      },
      {
        "text": "```{.cpp file=fft_multiply} vector\n\nfft(fa, false);\nfft(fb, false);\nfor (int i = 0; i < n; i++)\n    fa[i] *= fb[i];\nfft(fa, true);\n\nvector<int> result(n);\nfor (int i = 0; i < n; i++)\n    result[i] = round(fa[i].real());\nreturn result;\n\n} ```\n\nThis function works with polynomials with integer coefficients, however you can also adjust it to work with other types. Since there is some error when working with complex numbers, we need round the resulting coefficients at the end.",
        "score": 0.8944154977798462,
        "is_relevant": true
      },
      {
        "text": "variations that can perform arbitrary large polynomial/integer multiplications. E.g. in 1971 Schönhage and Strasser developed a variation for multiplying arbitrary large numbers that applies the FFT recursively in rings structures running in $O(n \\log n \\log \\log n)$. And recently (in 2019) Harvey and van der Hoeven published an algorithm that runs in true $O(n \\log n)$.",
        "score": 0.9114245176315308,
        "is_relevant": true
      },
      {
        "text": "The discrete Fourier transform (DFT) of the polynomial $A(x)$ (or equivalently the vector of coefficients $(a_0, a_1, \\dots, a_{n-1})$ is defined as the values of the polynomial at the points $x = w_{n, k}$, i.e. it is the vector:\n\n$$\\begin{align} \\text{DFT}(a_0, a_1, \\dots, a_{n-1}) &= (y_0, y_1, \\dots, y_{n-1}) \\ &= (A(w_{n, 0}), A(w_{n, 1}), \\dots, A(w_{n, n-1})) \\ &= (A(w_n^0), A(w_n^1), \\dots, A(w_n^{n-1})) \\end{align}$$",
        "score": 0.928287148475647,
        "is_relevant": true
      }
    ]
  },
  {
    "query": "Circle-line intersection algorithm",
    "chunk_scores": [
      {
        "text": "Fast application of a set of geometric operations to a set of points",
        "score": 1.0794148445129395,
        "is_relevant": false
      },
      {
        "text": "Thus, as an initial approximation for $z[i]$ we can safely take:\n\n$$ z_0[i] = \\min(r - i,\\; z[i-l]) $$\n\nAfter having $z[i]$ initialized to $z_0[i]$, we try to increment $z[i]$ by running the trivial algorithm -- because in general, after the border $r$, we cannot know if the segment will continue to match or not.",
        "score": 1.0998246669769287,
        "is_relevant": false
      },
      {
        "text": "As in algorithm 1, we first gave a simplified implementation that looks for only a numerical answer without finding the boundaries of the desired segment:\n\n```cpp int ans = a[0], sum = 0;\n\nfor (int r = 0; r < n; ++r) { sum += a[r]; ans = max(ans, sum); sum = max(sum, 0); } ```\n\nA complete solution, maintaining the indexes of the boundaries of the corresponding segment:\n\n```cpp int ans = a[0], ans_l = 0, ans_r = 0; int sum = 0, minus_pos = -1;",
        "score": 1.103184700012207,
        "is_relevant": false
      },
      {
        "text": "Now we give a full version of the solution, which additionally also finds the boundaries of the desired segment:\n\n```cpp int ans = a[0], ans_l = 0, ans_r = 0; int sum = 0, min_sum = 0, min_pos = -1;\n\nfor (int r = 0; r < n; ++r) { sum += a[r]; int cur = sum - min_sum; if (cur > ans) { ans = cur; ans_l = min_pos + 1; ans_r = r; } if (sum < min_sum) { min_sum = sum; min_pos = r; } } ```\n\nAlgorithm 2",
        "score": 1.121833086013794,
        "is_relevant": false
      },
      {
        "text": "Implementation\n\n```cpp struct Edge { int a, b, cost; };\n\nint n; vector\n\nvoid solve() { vector\n\nfor (int i = 0; i < n; ++i) {\n    x = -1;\n    for (Edge e : edges) {\n        if (d[e.a] + e.cost < d[e.b]) {\n            d[e.b] = max(-INF, d[e.a] + e.cost);\n            p[e.b] = e.a;\n            x = e.b;\n        }\n    }\n}\n\nif (x == -1) {\n    cout << \"No negative cycle found.\";\n} else {\n    for (int i = 0; i < n; ++i)\n        x = p[x];",
        "score": 1.1325163841247559,
        "is_relevant": false
      },
      {
        "text": "cout << \"Negative cycle: \";\n    for (int v : cycle)\n        cout << v << ' ';\n    cout << endl;\n}\n\n} ```\n\nUsing Floyd-Warshall algorithm\n\nThe Floyd-Warshall algorithm allows to solve the second variation of the problem - finding all pairs of vertices $(i, j)$ which don't have a shortest path between them (i.e. a path of arbitrarily small weight exists).\n\nAgain, the details can be found in the Floyd-Warshall article, and here we describe only its application.",
        "score": 1.145953893661499,
        "is_relevant": false
      },
      {
        "text": "void dfs2(int v, int cl) {\n    comp[v] = cl;\n    for (int u : adj_t[v]) {\n        if (comp[u] == -1)\n            dfs2(u, cl);\n    }\n}\n\nbool solve_2SAT() {\n    order.clear();\n    used.assign(n_vertices, false);\n    for (int i = 0; i < n_vertices; ++i) {\n        if (!used[i])\n            dfs1(i);\n    }\n\n    comp.assign(n_vertices, -1);\n    for (int i = 0, j = 0; i < n_vertices; ++i) {\n        int v = order[n_vertices - i - 1];\n        if (comp[v] == -1)\n            dfs2(v, j++);\n    }",
        "score": 1.1690924167633057,
        "is_relevant": false
      },
      {
        "text": "```{.cpp file=divide_and_conquer_dp} int m, n; vector\n\nlong long C(int i, int j);\n\n// compute dp_cur[l], ... dp_cur[r] (inclusive) void compute(int l, int r, int optl, int optr) { if (l > r) return;\n\nint mid = (l + r) >> 1;\npair<long long, int> best = {LLONG_MAX, -1};\n\nfor (int k = optl; k <= min(mid, optr); k++) {\n    best = min(best, {(k ? dp_before[k - 1] : 0) + C(k, mid), k});\n}\n\ndp_cur[mid] = best.first;\nint opt = best.second;\n\ncompute(l, mid - 1, optl, opt);\ncompute(mid + 1, r, opt, optr);",
        "score": 1.179142713546753,
        "is_relevant": false
      },
      {
        "text": "However, this is not enough to prove the algorithm. In the algorithm, we are actually limited in finding the answer only to such segments that begin immediately after the places when $s<0$ happened.",
        "score": 1.1943469047546387,
        "is_relevant": false
      },
      {
        "text": "Inside the loop for $i = 1 \\dots n - 1$ we first determine the initial value $z[i]$ -- it will either remain zero or be computed using the above formula.\n\nThereafter, the trivial algorithm attempts to increase the value of $z[i]$ as much as possible.\n\nIn the end, if it's required (that is, if $i + z[i] > r$), we update the rightmost match segment $[l, r)$.\n\nAsymptotic behavior of the algorithm",
        "score": 1.1973130702972412,
        "is_relevant": false
      }
    ]
  },
  {
    "query": "Kirchhoff's theorem in graph theory",
    "chunk_scores": [
      {
        "text": "The following formula was proven by Kirchhoff in 1847.\n\nKirchhoff's matrix tree theorem\n\nLet $A$ be the adjacency matrix of the graph: $A_{u,v}$ is the number of edges between $u$ and $v$. Let $D$ be the degree matrix of the graph: a diagonal matrix with $D_{u,u}$ being the degree of vertex $u$ (including multiple edges and loops - edges which connect vertex $u$ with itself).",
        "score": 0.7750582098960876,
        "is_relevant": true
      },
      {
        "text": "The determinant of the matrix can be found in $O(N^3)$ by using the Gaussian method.\n\nThe proof of this theorem is quite difficult and is not presented here; for an outline of the proof and variations of the theorem for graphs without multiple edges and for directed graphs refer to Wikipedia.\n\nRelation to Kirchhoff's circuit laws",
        "score": 0.8523809909820557,
        "is_relevant": true
      },
      {
        "text": "Relation to Kirchhoff's circuit laws\n\nKirchhoff's matrix tree theorem and Kirchhoff's laws for electrical circuit are related in a beautiful way. It is possible to show (using Ohm's law and Kirchhoff's first law) that resistance $R_{ij}$ between two points of the circuit $i$ and $j$ is\n\n$$R_{ij} = \\frac{ \\left| L^{(i,j)} \\right| }{ | L^j | }.$$",
        "score": 0.8969398736953735,
        "is_relevant": true
      },
      {
        "text": "tags: - Translated e_maxx_link: kirchhoff_theorem\n\nKirchhoff's theorem. Finding the number of spanning trees\n\nProblem: You are given a connected undirected graph (with possible multiple edges) represented using an adjacency matrix. Find the number of different spanning trees of this graph.\n\nThe following formula was proven by Kirchhoff in 1847.\n\nKirchhoff's matrix tree theorem",
        "score": 0.9411935210227966,
        "is_relevant": true
      },
      {
        "text": "Kirchhoff's theorem gives this formula geometric meaning.\n\nPractice Problems\n\nCODECHEF: Roads in Stars\n\nSPOJ: Maze\n\nCODECHEF: Complement Spanning Trees",
        "score": 0.9438744187355042,
        "is_relevant": true
      },
      {
        "text": "$$R_{ij} = \\frac{ \\left| L^{(i,j)} \\right| }{ | L^j | }.$$\n\nHere the matrix $L$ is obtained from the matrix of inverse resistances $A$ ($A_{i,j}$ is inverse of the resistance of the conductor between points $i$ and $j$) using the procedure described in Kirchhoff's matrix tree theorem. $T^j$ is the matrix with row and column $j$ removed, $T^{(i,j)}$ is the matrix with two rows and two columns $i$ and $j$ removed.\n\nKirchhoff's theorem gives this formula geometric meaning.\n\nPractice Problems",
        "score": 1.0091708898544312,
        "is_relevant": true
      },
      {
        "text": "In case $T$ doesn't contain the edge $e$, then $T + e$ will contain a cycle $C$. This cycle will contain at least one edge $f$, that is not in $F$. The set of edges $T - f + e$ will also be a spanning tree. Notice that the weight of $f$ cannot be smaller than the weight of $e$, because otherwise Kruskal would have chosen $f$ earlier. It also cannot have a bigger weight, since that would make the total weight of $T - f + e$ smaller than the total weight of $T$, which is impossible since $T$ is already a",
        "score": 1.0500330924987793,
        "is_relevant": false
      },
      {
        "text": "If the original graph was connected, then also the resulting graph will be connected. Because otherwise there would be two components that could be connected with at least one edge. Though this is impossible, because Kruskal would have chosen one of these edges, since the ids of the components are different. Also the resulting graph doesn't contain any cycles, since we forbid this explicitly in the algorithm. Therefore the algorithm generates a spanning tree.",
        "score": 1.0536680221557617,
        "is_relevant": false
      },
      {
        "text": "tags: - Translated e_maxx_link: mst_kruskal\n\nMinimum spanning tree - Kruskal's algorithm\n\nGiven a weighted undirected graph. We want to find a subtree of this graph which connects all vertices (i.e. it is a spanning tree) and has the least weight (i.e. the sum of weights of all the edges is minimum) of all possible spanning trees. This spanning tree is called a minimum spanning tree.",
        "score": 1.121764898300171,
        "is_relevant": false
      },
      {
        "text": "First we prove that the vertex $x$ cannot reach the vertex $\\lnot x$. Because we assigned $\\text{true}$ it has to hold that the index of strongly connected component of $x$ is greater than the index of the component of $\\lnot x$. This means that $\\lnot x$ is located on the left of the component containing $x$, and the later vertex cannot reach the first.",
        "score": 1.1330013275146484,
        "is_relevant": false
      }
    ]
  },
  {
    "query": "Counting labeled trees",
    "chunk_scores": [
      {
        "text": "Let $d_1, \\dots, d_k$ be the degrees of the vertices in the tree after connecting the vertices. The sum of the degrees is twice the number of edges:\n\n$$\\sum_{i=1}^k d_i = 2k - 2$$\n\nIf the vertex $i$ has degree $d_i$, then it appears $d_i - 1$ times in the Prüfer code. The Prüfer code for a tree with $k$ vertices has length $k-2$. So the number of ways to choose a code with $k-2$ numbers where the number $i$ appears exactly $d_i - 1$ times is equal to the multinomial coefficient",
        "score": 0.8538435101509094,
        "is_relevant": true
      },
      {
        "text": "Thus in order to calculate the number of possible ways it is important to count how often each of the $k$ vertices is used in the connecting tree. To obtain a formula for the problem it is necessary to sum the answer over all possible degrees.\n\nLet $d_1, \\dots, d_k$ be the degrees of the vertices in the tree after connecting the vertices. The sum of the degrees is twice the number of edges:\n\n$$\\sum_{i=1}^k d_i = 2k - 2$$",
        "score": 0.8728667497634888,
        "is_relevant": true
      },
      {
        "text": "Based on the formula from the previous section, we will learn how to count the number of labeled graphs with $n$ vertices and $k$ connected components.\n\nThis number can be computed using dynamic programming. We will compute $D[i][j]$ - the number of labeled graphs with $i$ vertices and $j$ components - for each $i \\le n$ and $j \\le k$.",
        "score": 0.8983355760574341,
        "is_relevant": false
      },
      {
        "text": "tags: - Translated e_maxx_link: prufer_code_cayley_formula\n\nPrüfer code\n\nIn this article we will look at the so-called Prüfer code (or Prüfer sequence), which is a way of encoding a labeled tree into a sequence of numbers in a unique way.\n\nWith the help of the Prüfer code we will prove Cayley's formula (which specified the number of spanning trees in a complete graph). Also we show the solution to the problem of counting the number of ways of adding edges to a graph to make it connected.",
        "score": 0.9010229706764221,
        "is_relevant": true
      },
      {
        "text": "Thus the Prüfer code for a given tree is a sequence of $n - 2$ numbers, where each number is the number of the connected vertex, i.e. this number is in the interval $[0, n-1]$.\n\nThe algorithm for computing the Prüfer code can be implemented easily with $O(n \\log n)$ time complexity, simply by using a data structure to extract the minimum (for instance set or priority_queue in C++), which contains a list of all the current leafs.\n\n```{.cpp file=pruefer_code_slow} vector\n\nvector",
        "score": 0.9222965240478516,
        "is_relevant": true
      },
      {
        "text": "Applying to this task the same idea it is possible to obtain this solution: we can implement a DFS, which will return a pointer to a set of integers - the list of numbers in that subtree. Then to get the answer for the current node (unless of course it is a leaf), we call DFS for all children of that node, and merge all the received sets together. The size of the resulting set will be the answer for the current node. To efficiently combine multiple sets we just apply the above-described recipe: we merge",
        "score": 0.9449001550674438,
        "is_relevant": true
      },
      {
        "text": "Since any labeled graph is uniquely determined by its edges, the number of labeled graphs with $n$ vertices is equal to:\n\n$$G_n = 2^{\\frac{n(n-1)}{2}}$$\n\nConnected labeled graphs\n\nHere, we additionally impose the restriction that the graph has to be connected.\n\nLet's denote the required number of connected graphs with $n$ vertices as $C_n$.",
        "score": 0.949063777923584,
        "is_relevant": false
      },
      {
        "text": "This algorithm can be implemented easily in $O(n \\log n)$: we use a data structure that supports extracting the minimum (for example set<> or priority_queue<> in C++) to store all the leaf vertices.\n\nThe following implementation returns the list of edges corresponding to the tree.\n\n```{.cpp file=pruefer_decode_slow} vector\n\nset<int> leaves;\nfor (int i = 0; i < n; i++) {\n    if (degree[i] == 1)\n        leaves.insert(i);\n}",
        "score": 0.9503844976425171,
        "is_relevant": false
      },
      {
        "text": "We consider the set of all possible edges of the graph. For each edge $(i, j)$ we can assume that $i < j$ (because the graph is undirected, and there are no loops). Therefore the set of all edges has the cardinality $\\binom{n}{2}$, i.e. $\\frac{n(n-1)}{2}$.\n\nSince any labeled graph is uniquely determined by its edges, the number of labeled graphs with $n$ vertices is equal to:\n\n$$G_n = 2^{\\frac{n(n-1)}{2}}$$\n\nConnected labeled graphs",
        "score": 0.9543353915214539,
        "is_relevant": false
      },
      {
        "text": "In the following image you can see the representation of such trees.\n\nExample-image of the set representation with trees\n\nIn the beginning, every element starts as a single set, therefore each vertex is its own tree. Then we combine the set containing the element 1 and the set containing the element 2. Then we combine the set containing the element 3 and the set containing the element 4. And in the last step, we combine the set containing the element 1 and the set containing the element 3.",
        "score": 0.9546709060668945,
        "is_relevant": false
      }
    ]
  },
  {
    "query": "String hashing techniques for long strings",
    "chunk_scores": [
      {
        "text": "This is demonstrated succinctly below:\n\ncpp long long get_diff_strings(){ long long tot = 0; for(int i = 1; i < sz; i++) { tot += st[i].len - st[st[i].link].len; } return tot; }\n\nWhile this is also $O(length(S))$, it requires no extra space and no recursive calls, consequently running faster in practice.\n\nTotal length of all different substrings\n\nGiven a string $S$. We want to compute the total length of all its various substrings.",
        "score": 0.8631973266601562,
        "is_relevant": false
      },
      {
        "text": "long long num_strings = longest - shortest + 1;\n    long long cur = num_strings * (longest + shortest) / 2;\n    tot += cur;\n}\nreturn tot;\n\n} ```\n\nThis approach runs in $O(length(S))$ time, but experimentally runs 20x faster than the memoized dynamic programming version on randomized strings. It requires no extra space and no recursion.\n\nLexicographically $k$-th substring {data-toc-label=\"Lexicographically k-th substring\"}",
        "score": 0.893202543258667,
        "is_relevant": false
      },
      {
        "text": "We construct a suffix automaton for the string $S$.\n\nWe will now take the string $T$, and for each prefix look for the longest suffix of this prefix in $S$. In other words, for each position in the string $T$, we want to find the longest common substring of $S$ and $T$ ending in that position.\n\nFor this we will use two variables, the current state $v$, and the current length $l$. These two variables will describe the current matching part: its length and the state that corresponds to it.",
        "score": 0.9045689105987549,
        "is_relevant": false
      },
      {
        "text": "So, we have found that the number of new substrings that appear when symbol $c$ is appended to $s$ is equal to $\\operatorname{length}(t) - z_{max}$.\n\nConsequently, the running time of this solution is $O(n^2)$ for a string of length $n$.\n\nIt's worth noting that in exactly the same way we can recalculate, still in $O(n)$ time, the number of distinct substrings when appending a character in the beginning of the string, as well as when removing it (from the end or the beginning).\n\nString compression",
        "score": 0.9371294975280762,
        "is_relevant": false
      },
      {
        "text": "The answer to the task will be the maximum of all the values $l$.\n\nThe complexity of this part is $O(length(T))$, since in one move we can either increase $l$ by one, or make several passes through the suffix links, each one ends up reducing the value $l$.\n\nImplementation:\n\n```cpp string lcs (string S, string T) { sa_init(); for (int i = 0; i < S.size(); i++) sa_extend(S[i]);",
        "score": 0.9873647689819336,
        "is_relevant": false
      },
      {
        "text": "```cpp long long get_tot_len_diff_substings() { long long tot = 0; for(int i = 1; i < sz; i++) { long long shortest = st[st[i].link].len + 1; long long longest = st[i].len;\n\n    long long num_strings = longest - shortest + 1;\n    long long cur = num_strings * (longest + shortest) / 2;\n    tot += cur;\n}\nreturn tot;\n\n} ```",
        "score": 1.0099105834960938,
        "is_relevant": false
      },
      {
        "text": "It is clear that this will take $O(length(P))$ time for each string $P$. Moreover the algorithm actually finds the length of the longest prefix of $P$ that appears in the text.\n\nNumber of different substrings\n\nGiven a string $S$. You want to compute the number of different substrings.\n\nLet us build a suffix automaton for the string $S$.",
        "score": 1.0107953548431396,
        "is_relevant": false
      },
      {
        "text": "A solution is: compute the Z-function of $s$, loop through all $i$ such that $i$ divides $n$. Stop at the first $i$ such that $i + z[i] = n$. Then, the string $s$ can be compressed to the length $i$.\n\nThe proof for this fact is the same as the solution which uses the prefix function.\n\nPractice Problems\n\neolymp - Blocks of string\n\nCodeforces - Password [Difficulty: Easy]\n\nUVA # 455 \"Periodic Strings\" [Difficulty: Medium]\n\nUVA # 11022 \"String Factoring\" [Difficulty: Medium]\n\nUVa 11475 - Extend to Palindrome",
        "score": 1.0201122760772705,
        "is_relevant": false
      },
      {
        "text": "This bound can also be achieved with the string:\n\n$$\\text{\"abbb}\\dots \\text{bbbc\"}$$\n\nApplications\n\nHere we look at some tasks that can be solved using the suffix automaton. For the simplicity we assume that the alphabet size $k$ is constant, which allows us to consider the complexity of appending a character and the traversal as constant.\n\nCheck for occurrence\n\nGiven a text $T$, and multiple patterns $P$. We have to check whether or not the strings $P$ appear as a substring of $T$.",
        "score": 1.0410832166671753,
        "is_relevant": false
      },
      {
        "text": "String compression\n\nGiven a string $s$ of length $n$. Find its shortest \"compressed\" representation, that is: find a string $t$ of shortest length such that $s$ can be represented as a concatenation of one or more copies of $t$.\n\nA solution is: compute the Z-function of $s$, loop through all $i$ such that $i$ divides $n$. Stop at the first $i$ such that $i + z[i] = n$. Then, the string $s$ can be compressed to the length $i$.",
        "score": 1.0458564758300781,
        "is_relevant": false
      }
    ]
  }
]